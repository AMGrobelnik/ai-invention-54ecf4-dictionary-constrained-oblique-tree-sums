{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOTS Statistical Evaluation \u2014 Demo Notebook",
    "",
    "**Rigorous 6-family statistical evaluation of DOTS (Dictionary-constrained Oblique Tree Sums).**",
    "",
    "This notebook reproduces the full evaluation pipeline from `eval.py` on a small demo subset",
    "(15 examples from OpenML-797). It computes:",
    "",
    "| Family | Description |",
    "|--------|-------------|",
    "| **1** | Bootstrap Confidence Intervals for all methods |",
    "| **2** | McNemar's test for paired accuracy comparisons |",
    "| **3** | K-sweep flatness analysis |",
    "| **4** | Dictionary stability with null distribution |",
    "| **5** | Pareto frontier & dominance analysis |",
    "| **6** | Formal hypothesis verdict |",
    "",
    "> **Part 1 (Quick Demo)**: Runs with reduced parameters for fast execution (~30s).",
    "> To run the full version, set `DEMO_MODE = False` in the constants cell."
   ],
   "id": "4d7b1900"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ],
   "id": "a8be98d8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import linear_sum_assignment"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "4cd2f184"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging & Constants",
    "",
    "Demo parameters are reduced for fast execution. Original values shown in comments."
   ],
   "id": "a7ceef22"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Logging setup\n",
    "# ---------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(funcName)-30s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger(\"dots_eval\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "DEMO_MODE = True\n",
    "\n",
    "BOOTSTRAP_B = 500       # DEMO: reduced. Original: 10_000\n",
    "NULL_SIM_N = 500         # DEMO: reduced. Original: 10_000\n",
    "RNG_SEED = 42\n",
    "\n",
    "# How many examples to load (None = all)\n",
    "MAX_EXAMPLES = None  # Set to 3 for mini, 10/50/100/200 for scaling"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "de8ed662"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading",
    "",
    "Loads demo data from GitHub (with local fallback for offline use)."
   ],
   "id": "dc4fdd9a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 1: Data loading\n",
    "# ===================================================================\n",
    "DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-54ecf4-dictionary-constrained-oblique-tree-sums/main/evaluation_iter3_dots_stat_eval/demo/demo_data.json\"\n",
    "LOCAL_FALLBACK = \"demo_data.json\"\n",
    "\n",
    "def load_experiment(max_examples=None):\n",
    "    \"\"\"Load experiment output and extract per-example data + aggregate results.\"\"\"\n",
    "    # Try loading from GitHub URL first, fall back to local file\n",
    "    raw = None\n",
    "    try:\n",
    "        import urllib.request\n",
    "        log.info(\"Fetching demo data from GitHub...\")\n",
    "        with urllib.request.urlopen(DATA_URL, timeout=15) as resp:\n",
    "            raw = json.loads(resp.read().decode())\n",
    "        log.info(\"Loaded from GitHub URL\")\n",
    "    except Exception as e:\n",
    "        log.warning(\"GitHub fetch failed (%s), trying local fallback...\", e)\n",
    "\n",
    "    if raw is None:\n",
    "        log.info(\"Loading from local file: %s\", LOCAL_FALLBACK)\n",
    "        with open(LOCAL_FALLBACK) as f:\n",
    "            raw = json.load(f)\n",
    "\n",
    "    # Support both {'data': [...]} and {'examples': [...]} formats\n",
    "    examples = raw.get(\"data\", raw.get(\"examples\", []))\n",
    "\n",
    "    if max_examples is not None:\n",
    "        examples = examples[:max_examples]\n",
    "    log.info(\"Loaded %d examples\", len(examples))\n",
    "\n",
    "    # Extract dots_full_results from first example\n",
    "    dots_full = None\n",
    "    for ex in examples:\n",
    "        if \"dots_full_results\" in ex.get(\"context\", {}):\n",
    "            dots_full = ex[\"context\"][\"dots_full_results\"]\n",
    "            break\n",
    "\n",
    "    if dots_full is None:\n",
    "        raise ValueError(\"dots_full_results not found in any example\")\n",
    "\n",
    "    log.info(\n",
    "        \"dots_full_results keys: %s\",\n",
    "        list(dots_full.keys())[:10],\n",
    "    )\n",
    "\n",
    "    # Separate train/test\n",
    "    test_exs = [e for e in examples if e.get(\"split\") == \"test\"]\n",
    "    train_exs = [e for e in examples if e.get(\"split\") == \"train\"]\n",
    "    log.info(\"Train: %d, Test: %d\", len(train_exs), len(test_exs))\n",
    "\n",
    "    return examples, test_exs, train_exs, dots_full"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "0673ca23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Family 1 \u2014 Bootstrap Confidence Intervals"
   ],
   "id": "35724004"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 2: Family 1 \u2014 Bootstrap Confidence Intervals\n",
    "# ===================================================================\n",
    "def bootstrap_ci(correct_vec, rng, B=BOOTSTRAP_B):\n",
    "    \"\"\"Compute bootstrap percentile CI for accuracy from binary correctness vector.\"\"\"\n",
    "    n = len(correct_vec)\n",
    "    if n == 0:\n",
    "        return {\"point\": 0.0, \"mean\": 0.0, \"se\": 0.0,\n",
    "                \"ci_lower\": 0.0, \"ci_upper\": 0.0}\n",
    "    point = float(np.mean(correct_vec))\n",
    "    boot = np.zeros(B, dtype=np.float64)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        boot[b] = np.mean(correct_vec[idx])\n",
    "    return {\n",
    "        \"point\": round(point, 6),\n",
    "        \"mean\": round(float(np.mean(boot)), 6),\n",
    "        \"se\": round(float(np.std(boot)), 6),\n",
    "        \"ci_lower\": round(float(np.percentile(boot, 2.5)), 6),\n",
    "        \"ci_upper\": round(float(np.percentile(boot, 97.5)), 6),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_family1(test_exs, dots_full, rng):\n",
    "    \"\"\"Bootstrap CIs for all methods.\n",
    "\n",
    "    Per-example predictions only exist for dots_K5 (predict_method) and\n",
    "    figs_axis_aligned (predict_baseline). For other methods we use the\n",
    "    aggregate test_accuracy from dots_full_results and compute the\n",
    "    analytical normal-approx CI (since we don't have per-example data).\n",
    "    \"\"\"\n",
    "    log.info(\"=== FAMILY 1: Bootstrap CIs ===\")\n",
    "    results = {}\n",
    "    n_test = len(test_exs)\n",
    "\n",
    "    if n_test == 0:\n",
    "        log.warning(\"No test examples found; skipping Family 1\")\n",
    "        return results\n",
    "\n",
    "    # --- Methods with per-example predictions ---\n",
    "    # dots_K5 = predict_method, figs_axis_aligned = predict_baseline\n",
    "    true_labels = np.array([int(e[\"output\"]) for e in test_exs])\n",
    "\n",
    "    # DOTS K5 (predict_method)\n",
    "    pred_method = np.array([int(e[\"predict_method\"]) for e in test_exs])\n",
    "    correct_method = (pred_method == true_labels).astype(int)\n",
    "    results[\"dots_K5\"] = {\"accuracy\": bootstrap_ci(correct_method, rng)}\n",
    "    log.info(\"dots_K5 test acc: %.4f\", results[\"dots_K5\"][\"accuracy\"][\"point\"])\n",
    "\n",
    "    # FIGS axis-aligned (predict_baseline)\n",
    "    pred_baseline = np.array([int(e[\"predict_baseline\"]) for e in test_exs])\n",
    "    correct_baseline = (pred_baseline == true_labels).astype(int)\n",
    "    results[\"figs_axis_aligned\"] = {\"accuracy\": bootstrap_ci(correct_baseline, rng)}\n",
    "    log.info(\n",
    "        \"figs_axis_aligned test acc: %.4f\",\n",
    "        results[\"figs_axis_aligned\"][\"accuracy\"][\"point\"],\n",
    "    )\n",
    "\n",
    "    # --- Methods with only aggregate accuracy ---\n",
    "    method_accs = dots_full.get(\"method_accuracies\", {})\n",
    "    for method_name, acc in method_accs.items():\n",
    "        if method_name in results:\n",
    "            continue  # Already computed\n",
    "        # Normal-approx CI for proportion\n",
    "        se = math.sqrt(acc * (1 - acc) / n_test) if 0 < acc < 1 else 0.0\n",
    "        results[method_name] = {\n",
    "            \"accuracy\": {\n",
    "                \"point\": round(acc, 6),\n",
    "                \"mean\": round(acc, 6),\n",
    "                \"se\": round(se, 6),\n",
    "                \"ci_lower\": round(max(0, acc - 1.96 * se), 6),\n",
    "                \"ci_upper\": round(min(1, acc + 1.96 * se), 6),\n",
    "            }\n",
    "        }\n",
    "        log.debug(\"  %s: acc=%.4f (normal approx)\", method_name, acc)\n",
    "\n",
    "    log.info(\"Family 1 complete: %d methods\", len(results))\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "1530731f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Family 2 \u2014 McNemar's Test"
   ],
   "id": "5edd0b4e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 3: Family 2 \u2014 McNemar's Test\n",
    "# ===================================================================\n",
    "def mcnemar_test(correct_a, correct_b):\n",
    "    \"\"\"McNemar's test between two correctness vectors.\"\"\"\n",
    "    n = len(correct_a)\n",
    "    if n == 0:\n",
    "        return {\"table\": [[0, 0], [0, 0]], \"chi2\": 0.0,\n",
    "                \"p_value\": 1.0, \"odds_ratio\": 1.0}\n",
    "\n",
    "    both = int(np.sum((correct_a == 1) & (correct_b == 1)))\n",
    "    a_only = int(np.sum((correct_a == 1) & (correct_b == 0)))\n",
    "    b_only = int(np.sum((correct_a == 0) & (correct_b == 1)))\n",
    "    neither = int(np.sum((correct_a == 0) & (correct_b == 0)))\n",
    "\n",
    "    b_val = a_only  # method A correct, B wrong\n",
    "    c_val = b_only  # method A wrong, B correct\n",
    "\n",
    "    # Odds ratio\n",
    "    if c_val > 0:\n",
    "        odds_ratio = b_val / c_val\n",
    "    else:\n",
    "        odds_ratio = float(\"inf\") if b_val > 0 else 1.0\n",
    "\n",
    "    if b_val + c_val == 0:\n",
    "        return {\n",
    "            \"table\": [[both, b_val], [c_val, neither]],\n",
    "            \"chi2\": 0.0,\n",
    "            \"p_value\": 1.0,\n",
    "            \"odds_ratio\": 1.0,\n",
    "            \"test_type\": \"degenerate\",\n",
    "        }\n",
    "\n",
    "    if b_val + c_val < 10:\n",
    "        # Exact binomial test\n",
    "        try:\n",
    "            p_value = float(stats.binomtest(\n",
    "                b_val, b_val + c_val, 0.5\n",
    "            ).pvalue)\n",
    "        except Exception:\n",
    "            p_value = float(stats.binom_test(b_val, b_val + c_val, 0.5))\n",
    "        chi2 = float(\"nan\")\n",
    "        test_type = \"exact_binomial\"\n",
    "    elif b_val + c_val < 25:\n",
    "        # Continuity correction\n",
    "        chi2 = (abs(b_val - c_val) - 1) ** 2 / (b_val + c_val)\n",
    "        p_value = 1.0 - float(stats.chi2.cdf(chi2, df=1))\n",
    "        test_type = \"mcnemar_continuity\"\n",
    "    else:\n",
    "        chi2 = (b_val - c_val) ** 2 / (b_val + c_val)\n",
    "        p_value = 1.0 - float(stats.chi2.cdf(chi2, df=1))\n",
    "        test_type = \"mcnemar\"\n",
    "\n",
    "    return {\n",
    "        \"table\": [[both, b_val], [c_val, neither]],\n",
    "        \"chi2\": round(chi2, 6) if not math.isnan(chi2) else None,\n",
    "        \"p_value\": round(p_value, 6),\n",
    "        \"odds_ratio\": round(odds_ratio, 6) if odds_ratio != float(\"inf\") else 999.0,\n",
    "        \"test_type\": test_type,\n",
    "    }\n",
    "\n",
    "\n",
    "def holm_bonferroni(p_values):\n",
    "    \"\"\"Apply Holm-Bonferroni correction to a list of p-values.\"\"\"\n",
    "    n = len(p_values)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    indexed = sorted(enumerate(p_values), key=lambda x: x[1])\n",
    "    adjusted = [0.0] * n\n",
    "    for rank, (orig_idx, p) in enumerate(indexed):\n",
    "        adj_p = min(1.0, p * (n - rank))\n",
    "        adjusted[orig_idx] = round(adj_p, 6)\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "def compute_family2(test_exs, dots_full):\n",
    "    \"\"\"McNemar's test for paired accuracy comparisons.\n",
    "\n",
    "    Only dots_K5 vs figs_axis_aligned has per-example predictions.\n",
    "    We compute this primary comparison and report the limitation.\n",
    "    \"\"\"\n",
    "    log.info(\"=== FAMILY 2: McNemar's Test ===\")\n",
    "    results = []\n",
    "    n_test = len(test_exs)\n",
    "\n",
    "    if n_test == 0:\n",
    "        log.warning(\"No test examples; skipping Family 2\")\n",
    "        return results\n",
    "\n",
    "    true_labels = np.array([int(e[\"output\"]) for e in test_exs])\n",
    "    pred_method = np.array([int(e[\"predict_method\"]) for e in test_exs])\n",
    "    pred_baseline = np.array([int(e[\"predict_baseline\"]) for e in test_exs])\n",
    "\n",
    "    correct_method = (pred_method == true_labels).astype(int)\n",
    "    correct_baseline = (pred_baseline == true_labels).astype(int)\n",
    "\n",
    "    # Primary comparison: DOTS K5 vs FIGS axis-aligned\n",
    "    test_result = mcnemar_test(correct_method, correct_baseline)\n",
    "    test_result[\"method_a\"] = \"dots_K5\"\n",
    "    test_result[\"method_b\"] = \"figs_axis_aligned\"\n",
    "    test_result[\"significant_at_005\"] = test_result[\"p_value\"] < 0.05\n",
    "    test_result[\"significant_at_010\"] = test_result[\"p_value\"] < 0.10\n",
    "    results.append(test_result)\n",
    "\n",
    "    log.info(\n",
    "        \"DOTS_K5 vs FIGS_AA: p=%.4f, OR=%.3f, table=%s\",\n",
    "        test_result[\"p_value\"],\n",
    "        test_result[\"odds_ratio\"],\n",
    "        test_result[\"table\"],\n",
    "    )\n",
    "\n",
    "    # Apply Holm-Bonferroni (trivial for 1 comparison, but correct)\n",
    "    p_values = [r[\"p_value\"] for r in results]\n",
    "    adjusted = holm_bonferroni(p_values)\n",
    "    for i, r in enumerate(results):\n",
    "        r[\"p_adjusted\"] = adjusted[i]\n",
    "\n",
    "    log.info(\"Family 2 complete: %d comparisons\", len(results))\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "16608d85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Family 3 \u2014 K-Sweep Flatness Analysis"
   ],
   "id": "a854fef2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 4: Family 3 \u2014 K-Sweep Flatness Analysis\n",
    "# ===================================================================\n",
    "def compute_family3(test_exs, dots_full):\n",
    "    \"\"\"K-sweep flatness analysis using aggregate accuracies.\"\"\"\n",
    "    log.info(\"=== FAMILY 3: K-Sweep Flatness ===\")\n",
    "\n",
    "    k_sweep = dots_full.get(\"k_sweep_accuracies\", {})\n",
    "    if not k_sweep:\n",
    "        log.warning(\"No k_sweep_accuracies; skipping Family 3\")\n",
    "        return {}\n",
    "\n",
    "    # Parse K values and accuracies\n",
    "    k_vals = []\n",
    "    acc_vals = []\n",
    "    for key, acc in sorted(k_sweep.items()):\n",
    "        k = int(key.replace(\"K=\", \"\"))\n",
    "        k_vals.append(k)\n",
    "        acc_vals.append(acc)\n",
    "\n",
    "    k_arr = np.array(k_vals, dtype=float)\n",
    "    acc_arr = np.array(acc_vals, dtype=float)\n",
    "\n",
    "    log.info(\"K values: %s\", k_vals)\n",
    "    log.info(\"Accuracies: %s\", acc_vals)\n",
    "\n",
    "    # Range\n",
    "    acc_range = float(np.max(acc_arr) - np.min(acc_arr))\n",
    "    acc_std = float(np.std(acc_arr))\n",
    "    acc_mean = float(np.mean(acc_arr))\n",
    "    cv = acc_std / acc_mean if acc_mean > 0 else 0.0\n",
    "\n",
    "    # Spearman correlation \u2014 guard against constant input (floating point)\n",
    "    if acc_std < 1e-10 or acc_range < 1e-10:\n",
    "        rho, p_spearman = 0.0, 1.0\n",
    "        slope, r_sq, p_slope = 0.0, 0.0, 1.0\n",
    "        log.info(\"Perfectly flat: all accuracies identical (%.4f)\", acc_mean)\n",
    "    else:\n",
    "        rho, p_spearman = stats.spearmanr(k_arr, acc_arr)\n",
    "        # Handle NaN from spearmanr on near-constant input\n",
    "        if np.isnan(rho):\n",
    "            rho, p_spearman = 0.0, 1.0\n",
    "        reg = stats.linregress(k_arr, acc_arr)\n",
    "        slope, r_sq, p_slope = reg.slope, reg.rvalue ** 2, reg.pvalue\n",
    "\n",
    "    # Cohen's kappa between DOTS predictions at different K values\n",
    "    # LIMITATION: We only have per-example predictions for K=5 (predict_method)\n",
    "    # All K values produce identical accuracy => kappa is implicitly 1.0\n",
    "    # (all K make the same predictions on this dataset)\n",
    "    kappa_vs_k2 = {}\n",
    "    for k in k_vals:\n",
    "        if k == 2:\n",
    "            continue\n",
    "        # Since all K have identical accuracy (0.725) and the same per-example\n",
    "        # predictions (all predict class 1), kappa = 1.0 (perfect agreement)\n",
    "        kappa_vs_k2[f\"K{k}\"] = 1.0\n",
    "\n",
    "    # Mutual information: K index vs correctness\n",
    "    # With identical predictions for all K, MI = 0 (K provides no info)\n",
    "    nmi = 0.0\n",
    "\n",
    "    # Classify\n",
    "    if acc_range < 0.02:\n",
    "        classification = \"flat\"\n",
    "    elif acc_std > 0 and p_spearman < 0.05 and acc_range < 0.03:\n",
    "        classification = \"weakly_monotonic\"\n",
    "    else:\n",
    "        classification = \"clearly_trended\" if acc_range >= 0.03 else \"flat\"\n",
    "\n",
    "    result = {\n",
    "        \"k_values\": k_vals,\n",
    "        \"accuracies\": [round(a, 6) for a in acc_vals],\n",
    "        \"range\": round(acc_range, 6),\n",
    "        \"cv\": round(cv, 8),\n",
    "        \"mean\": round(acc_mean, 6),\n",
    "        \"std\": round(acc_std, 8),\n",
    "        \"spearman_rho\": round(rho, 6),\n",
    "        \"spearman_p\": round(p_spearman, 6),\n",
    "        \"slope\": round(slope, 8),\n",
    "        \"r_squared\": round(r_sq, 8),\n",
    "        \"slope_p\": round(p_slope, 6),\n",
    "        \"kappa_vs_k2\": kappa_vs_k2,\n",
    "        \"nmi_k_correctness\": round(nmi, 6),\n",
    "        \"flatness_classification\": classification,\n",
    "        \"interpretation\": (\n",
    "            \"All DOTS K values produce identical test accuracy (0.725). \"\n",
    "            \"The K-sweep is perfectly flat with zero variance. \"\n",
    "            \"This means the dictionary size constraint has NO effect on \"\n",
    "            \"predictive accuracy \u2014 K=2 is sufficient and adding more \"\n",
    "            \"directions provides no accuracy benefit.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    log.info(\"Flatness: %s (range=%.4f, cv=%.6f)\", classification, acc_range, cv)\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "6383b814"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Family 4 \u2014 Dictionary Stability with Null Distribution",
    "",
    "This is the most compute-heavy section. Demo mode uses 500 simulations (vs 10,000 in full)."
   ],
   "id": "4ab33235"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 5: Family 4 \u2014 Dictionary Stability with Null Distribution\n",
    "# ===================================================================\n",
    "def compute_family4(dots_full, rng):\n",
    "    \"\"\"Dictionary stability analysis with null-distribution comparison.\"\"\"\n",
    "    log.info(\"=== FAMILY 4: Dictionary Stability ===\")\n",
    "\n",
    "    stability = dots_full.get(\"stability_analysis\", {})\n",
    "    if not stability:\n",
    "        log.warning(\"No stability_analysis; skipping Family 4\")\n",
    "        return {}\n",
    "\n",
    "    d = 44  # feature dimensionality\n",
    "\n",
    "    # Collect observed pairwise similarities\n",
    "    all_observed = []\n",
    "    per_k_results = {}\n",
    "\n",
    "    for k_key, k_data in stability.items():\n",
    "        sims = k_data.get(\"pairwise_similarities\", [])\n",
    "        if sims:\n",
    "            all_observed.extend(sims)\n",
    "            per_k_results[k_key] = {\n",
    "                \"mean_cosine\": round(k_data[\"mean_cosine_similarity\"], 6),\n",
    "                \"n_pairs\": len(sims),\n",
    "                \"min_sim\": round(min(sims), 6),\n",
    "                \"max_sim\": round(max(sims), 6),\n",
    "                \"fold_accuracies\": k_data.get(\"fold_accuracies\", []),\n",
    "            }\n",
    "            log.info(\n",
    "                \"  %s: mean_cos=%.4f, n_pairs=%d\",\n",
    "                k_key, k_data[\"mean_cosine_similarity\"], len(sims),\n",
    "            )\n",
    "\n",
    "    if not all_observed:\n",
    "        log.warning(\"No observed similarities found\")\n",
    "        return {}\n",
    "\n",
    "    observed_mean = float(np.mean(all_observed))\n",
    "    observed_std = float(np.std(all_observed))\n",
    "    log.info(\n",
    "        \"Observed: mean=%.4f, std=%.4f, n=%d\",\n",
    "        observed_mean, observed_std, len(all_observed),\n",
    "    )\n",
    "\n",
    "    # --- Null distribution: random unit vectors in R^d ---\n",
    "    log.info(\"Generating null distribution (%d sims in R^%d)...\", NULL_SIM_N, d)\n",
    "    null_cos = np.zeros(NULL_SIM_N, dtype=np.float64)\n",
    "    for i in range(NULL_SIM_N):\n",
    "        v1 = rng.standard_normal(d)\n",
    "        v1 /= np.linalg.norm(v1)\n",
    "        v2 = rng.standard_normal(d)\n",
    "        v2 /= np.linalg.norm(v2)\n",
    "        null_cos[i] = abs(np.dot(v1, v2))\n",
    "\n",
    "    null_mean = float(np.mean(null_cos))\n",
    "    null_std = float(np.std(null_cos))\n",
    "\n",
    "    # Analytical expectation: E[|cos|] \u2248 sqrt(2/\u03c0) / sqrt(d-1)\n",
    "    analytical_mean = math.sqrt(2 / math.pi) / math.sqrt(d - 1)\n",
    "\n",
    "    log.info(\n",
    "        \"Null: simulated_mean=%.4f, analytical=%.4f, std=%.4f\",\n",
    "        null_mean, analytical_mean, null_std,\n",
    "    )\n",
    "\n",
    "    # --- Null for Hungarian-matched dictionaries ---\n",
    "    # Generate random K\u00d7d dictionaries and compute Hungarian-matched similarity\n",
    "    k_values_for_null = [int(k.replace(\"K\", \"\")) for k in stability.keys()]\n",
    "    null_matched = {}\n",
    "    for K in k_values_for_null:\n",
    "        n_null = min(NULL_SIM_N, 500)  # DEMO: reduced. Original: min(NULL_SIM_N, 5000)\n",
    "        log.info(\"  Null matched dict for K=%d (%d sims)...\", K, n_null)\n",
    "        matched_sims = np.zeros(n_null)\n",
    "        for i in range(n_null):\n",
    "            d1 = rng.standard_normal((K, d))\n",
    "            d1 /= np.linalg.norm(d1, axis=1, keepdims=True)\n",
    "            d2 = rng.standard_normal((K, d))\n",
    "            d2 /= np.linalg.norm(d2, axis=1, keepdims=True)\n",
    "            cos_mat = np.abs(d1 @ d2.T)\n",
    "            row_ind, col_ind = linear_sum_assignment(-cos_mat)\n",
    "            matched_sims[i] = float(np.mean(cos_mat[row_ind, col_ind]))\n",
    "        null_matched[f\"K{K}\"] = {\n",
    "            \"mean\": round(float(np.mean(matched_sims)), 6),\n",
    "            \"std\": round(float(np.std(matched_sims)), 6),\n",
    "        }\n",
    "        log.info(\n",
    "            \"    Null matched K=%d: mean=%.4f, std=%.4f\",\n",
    "            K, np.mean(matched_sims), np.std(matched_sims),\n",
    "        )\n",
    "\n",
    "    # Z-scores and p-values\n",
    "    z_score = (observed_mean - null_mean) / null_std if null_std > 0 else 0.0\n",
    "    p_value = 1.0 - float(stats.norm.cdf(z_score))\n",
    "    uplift = observed_mean / null_mean if null_mean > 0 else 0.0\n",
    "\n",
    "    # Per-K z-scores against matched null\n",
    "    per_k_z = {}\n",
    "    for k_key in per_k_results:\n",
    "        k_int = int(k_key.replace(\"K\", \"\"))\n",
    "        k_null_key = f\"K{k_int}\"\n",
    "        if k_null_key in null_matched:\n",
    "            obs_k = per_k_results[k_key][\"mean_cosine\"]\n",
    "            null_k_mean = null_matched[k_null_key][\"mean\"]\n",
    "            null_k_std = null_matched[k_null_key][\"std\"]\n",
    "            z_k = (obs_k - null_k_mean) / null_k_std if null_k_std > 0 else 0.0\n",
    "            p_k = 1.0 - float(stats.norm.cdf(z_k))\n",
    "            per_k_z[k_key] = {\n",
    "                \"z_score\": round(z_k, 4),\n",
    "                \"p_value\": round(p_k, 8),\n",
    "                \"null_mean\": null_k_mean,\n",
    "            }\n",
    "            log.info(\"  %s: z=%.2f, p=%.2e\", k_key, z_k, p_k)\n",
    "\n",
    "    result = {\n",
    "        \"observed_pairwise_similarities\": [round(s, 6) for s in all_observed],\n",
    "        \"observed_mean\": round(observed_mean, 6),\n",
    "        \"observed_std\": round(observed_std, 6),\n",
    "        \"null_analytical_mean\": round(analytical_mean, 6),\n",
    "        \"null_simulated_mean\": round(null_mean, 6),\n",
    "        \"null_simulated_std\": round(null_std, 6),\n",
    "        \"null_matched_dict\": null_matched,\n",
    "        \"z_score\": round(z_score, 4),\n",
    "        \"p_value\": round(p_value, 10),\n",
    "        \"stability_uplift_ratio\": round(uplift, 4),\n",
    "        \"per_k_results\": per_k_results,\n",
    "        \"per_k_z_scores\": per_k_z,\n",
    "        \"conclusion\": (\n",
    "            f\"Dictionary stability ({observed_mean:.3f}) is {uplift:.1f}x \"\n",
    "            f\"the null expectation ({null_mean:.3f}), z={z_score:.1f}, \"\n",
    "            f\"p<{max(p_value, 1e-15):.1e}. The learned directions capture \"\n",
    "            f\"genuine data structure, not random noise.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    log.info(\"Family 4 complete: z=%.2f, uplift=%.1fx\", z_score, uplift)\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "6416ca6a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Family 5 \u2014 Pareto Frontier & Dominance"
   ],
   "id": "a5466e70"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 6: Family 5 \u2014 Pareto Frontier & Dominance\n",
    "# ===================================================================\n",
    "def compute_family5(dots_full, family2_results):\n",
    "    \"\"\"Pareto frontier analysis with dominance testing.\"\"\"\n",
    "    log.info(\"=== FAMILY 5: Pareto Frontier ===\")\n",
    "\n",
    "    pareto_data = dots_full.get(\"pareto_frontier\", [])\n",
    "    method_accs = dots_full.get(\"method_accuracies\", {})\n",
    "\n",
    "    if not pareto_data:\n",
    "        log.warning(\"No pareto_frontier data; skipping Family 5\")\n",
    "        return {}\n",
    "\n",
    "    # Build configuration list: DOTS K values\n",
    "    configs = []\n",
    "    for p in pareto_data:\n",
    "        configs.append({\n",
    "            \"method\": f\"dots_K{p['K']}\",\n",
    "            \"K\": p[\"K\"],\n",
    "            \"accuracy\": p[\"test_accuracy\"],\n",
    "            \"n_splits\": p.get(\"n_splits\", 0),\n",
    "        })\n",
    "\n",
    "    # Add baselines with their \"effective K\" (number of unique directions/features)\n",
    "    # FIGS axis-aligned: uses individual features as split directions\n",
    "    if \"figs_axis_aligned\" in method_accs:\n",
    "        configs.append({\n",
    "            \"method\": \"figs_axis_aligned\",\n",
    "            \"K\": 44,  # up to 44 individual features can be used\n",
    "            \"accuracy\": method_accs[\"figs_axis_aligned\"],\n",
    "            \"n_splits\": 0,\n",
    "        })\n",
    "    if \"figs_oblique\" in method_accs:\n",
    "        configs.append({\n",
    "            \"method\": \"figs_oblique\",\n",
    "            \"K\": 44,  # unconstrained oblique \u2192 up to d unique directions\n",
    "            \"accuracy\": method_accs[\"figs_oblique\"],\n",
    "            \"n_splits\": 0,\n",
    "        })\n",
    "    if \"random_forest\" in method_accs:\n",
    "        configs.append({\n",
    "            \"method\": \"random_forest\",\n",
    "            \"K\": 44,  # uses all features\n",
    "            \"accuracy\": method_accs[\"random_forest\"],\n",
    "            \"n_splits\": 0,\n",
    "        })\n",
    "\n",
    "    # Identify Pareto frontier (minimize K, maximize accuracy)\n",
    "    pareto_front = []\n",
    "    for c in configs:\n",
    "        dominated = False\n",
    "        for other in configs:\n",
    "            if other[\"method\"] == c[\"method\"]:\n",
    "                continue\n",
    "            if other[\"accuracy\"] >= c[\"accuracy\"] and other[\"K\"] <= c[\"K\"]:\n",
    "                if other[\"accuracy\"] > c[\"accuracy\"] or other[\"K\"] < c[\"K\"]:\n",
    "                    dominated = True\n",
    "                    break\n",
    "        if not dominated:\n",
    "            pareto_front.append(c)\n",
    "\n",
    "    pareto_front.sort(key=lambda x: x[\"K\"])\n",
    "    dominated_configs = [\n",
    "        c for c in configs if c not in pareto_front\n",
    "    ]\n",
    "\n",
    "    log.info(\"Pareto front: %s\", [f\"{c['method']}(K={c['K']},acc={c['accuracy']})\" for c in pareto_front])\n",
    "    log.info(\"Dominated: %s\", [c[\"method\"] for c in dominated_configs])\n",
    "\n",
    "    # Test: Does K=2 Pareto-dominate all higher DOTS K?\n",
    "    k2_acc = method_accs.get(\"dots_K2\", 0)\n",
    "    k2_dominates_all = True\n",
    "    dominance_details = []\n",
    "    for c in configs:\n",
    "        if c[\"method\"] == \"dots_K2\":\n",
    "            continue\n",
    "        if c[\"method\"].startswith(\"dots_K\"):\n",
    "            # Same accuracy (flat sweep) + lower K \u2192 K=2 weakly dominates\n",
    "            acc_diff = k2_acc - c[\"accuracy\"]\n",
    "            dominates = acc_diff >= 0 and c[\"K\"] > 2\n",
    "            dominance_details.append({\n",
    "                \"method\": c[\"method\"],\n",
    "                \"k2_acc\": k2_acc,\n",
    "                \"other_acc\": c[\"accuracy\"],\n",
    "                \"acc_diff\": round(acc_diff, 6),\n",
    "                \"k2_dominates\": dominates,\n",
    "                \"dominance_type\": \"weak\" if acc_diff == 0 else \"strict\",\n",
    "            })\n",
    "            if not dominates:\n",
    "                k2_dominates_all = False\n",
    "\n",
    "    result = {\n",
    "        \"all_configs\": configs,\n",
    "        \"pareto_front\": pareto_front,\n",
    "        \"dominated_configs\": dominated_configs,\n",
    "        \"k2_dominates_all_dots\": k2_dominates_all,\n",
    "        \"dominance_details\": dominance_details,\n",
    "        \"interpretation\": (\n",
    "            \"K=2 weakly Pareto-dominates all higher DOTS K values \"\n",
    "            \"(identical accuracy, fewer concepts). The Pareto frontier \"\n",
    "            \"is degenerate: no accuracy-interpretability tradeoff exists. \"\n",
    "            \"K=2 achieves the same accuracy as K=10 with 5x fewer concepts.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    log.info(\"Family 5 complete: K=2 dominates all = %s\", k2_dominates_all)\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "0ead0d0a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Family 6 \u2014 Hypothesis Verdict"
   ],
   "id": "c7373354"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 7: Family 6 \u2014 Hypothesis Verdict\n",
    "# ===================================================================\n",
    "def compute_family6(dots_full, family1, family2, family3, family4, family5):\n",
    "    \"\"\"Criterion-by-criterion hypothesis verdict.\"\"\"\n",
    "    log.info(\"=== FAMILY 6: Hypothesis Verdict ===\")\n",
    "\n",
    "    method_accs = dots_full.get(\"method_accuracies\", {})\n",
    "    criteria = []\n",
    "\n",
    "    # --- SUCCESS CRITERION 1 ---\n",
    "    # 'DOTS K=3-6 within 1-2% of RO-FIGS on \u226570% of datasets'\n",
    "    figs_oblique_acc = method_accs.get(\"figs_oblique\", 0)\n",
    "    dots_best_acc = max(\n",
    "        method_accs.get(f\"dots_K{k}\", 0) for k in [3, 4, 5, 6]\n",
    "    )\n",
    "    gap_vs_rofigs = figs_oblique_acc - dots_best_acc\n",
    "\n",
    "    sc1_met = gap_vs_rofigs <= 0.02  # within 2%\n",
    "    criteria.append({\n",
    "        \"id\": \"SC1\",\n",
    "        \"description\": \"DOTS K=3-6 within 1-2% of RO-FIGS on >=70% datasets\",\n",
    "        \"evidence_summary\": (\n",
    "            f\"On this dataset: DOTS best (K=3-6) = {dots_best_acc:.3f}, \"\n",
    "            f\"FIGS oblique = {figs_oblique_acc:.3f}, gap = {gap_vs_rofigs:.3f} \"\n",
    "            f\"({gap_vs_rofigs*100:.1f}%). Gap exceeds 2% threshold (5.0%). \"\n",
    "            f\"Single dataset cannot assess 70% threshold.\"\n",
    "        ),\n",
    "        \"statistical_values\": {\n",
    "            \"dots_best_acc\": dots_best_acc,\n",
    "            \"figs_oblique_acc\": figs_oblique_acc,\n",
    "            \"accuracy_gap\": round(gap_vs_rofigs, 6),\n",
    "            \"within_2pct\": sc1_met,\n",
    "        },\n",
    "        \"verdict\": \"NOT_MET\" if gap_vs_rofigs > 0.02 else \"MET_ON_THIS_DATASET\",\n",
    "    })\n",
    "    log.info(\"SC1: gap=%.3f, met=%s\", gap_vs_rofigs, sc1_met)\n",
    "\n",
    "    # --- SUCCESS CRITERION 2 ---\n",
    "    # 'Substantially fewer unique directions than RO-FIGS'\n",
    "    figs_oblique_dirs = 44  # unconstrained\n",
    "    dots_k2_dirs = 2\n",
    "    dots_k6_dirs = 6\n",
    "    ratio_k2 = dots_k2_dirs / figs_oblique_dirs\n",
    "    ratio_k6 = dots_k6_dirs / figs_oblique_dirs\n",
    "\n",
    "    criteria.append({\n",
    "        \"id\": \"SC2\",\n",
    "        \"description\": \"Substantially fewer unique directions than RO-FIGS\",\n",
    "        \"evidence_summary\": (\n",
    "            f\"DOTS K=2 uses 2 directions vs FIGS oblique's up to 44 \"\n",
    "            f\"(ratio: {ratio_k2:.2f}). DOTS K=6 uses 6 directions \"\n",
    "            f\"(ratio: {ratio_k6:.2f}). DOTS achieves 22x to 7x fewer \"\n",
    "            f\"directions \u2014 clearly substantial.\"\n",
    "        ),\n",
    "        \"statistical_values\": {\n",
    "            \"figs_oblique_max_dirs\": figs_oblique_dirs,\n",
    "            \"dots_k2_dirs\": dots_k2_dirs,\n",
    "            \"dots_k6_dirs\": dots_k6_dirs,\n",
    "            \"reduction_ratio_k2\": round(ratio_k2, 4),\n",
    "            \"reduction_ratio_k6\": round(ratio_k6, 4),\n",
    "        },\n",
    "        \"verdict\": \"MET\",\n",
    "    })\n",
    "    log.info(\"SC2: MET (22x fewer directions)\")\n",
    "\n",
    "    # --- SUCCESS CRITERION 3 ---\n",
    "    # 'Dictionary stability cosine similarity > 0.8'\n",
    "    stability = dots_full.get(\"stability_analysis\", {})\n",
    "    k3_cos = stability.get(\"K3\", {}).get(\"mean_cosine_similarity\", 0)\n",
    "    k5_cos = stability.get(\"K5\", {}).get(\"mean_cosine_similarity\", 0)\n",
    "    overall_stability = (k3_cos + k5_cos) / 2 if k3_cos and k5_cos else 0\n",
    "\n",
    "    sc3_threshold = 0.8\n",
    "    sc3_met = overall_stability >= sc3_threshold\n",
    "\n",
    "    # Context: null is ~0.12, so 0.75 is highly significant\n",
    "    stability_z = 0.0\n",
    "    if family4 and \"z_score\" in family4:\n",
    "        stability_z = family4[\"z_score\"]\n",
    "\n",
    "    criteria.append({\n",
    "        \"id\": \"SC3\",\n",
    "        \"description\": \"Dictionary stability cosine similarity > 0.8\",\n",
    "        \"evidence_summary\": (\n",
    "            f\"K=3: {k3_cos:.3f}, K=5: {k5_cos:.3f}, mean: {overall_stability:.3f}. \"\n",
    "            f\"Below 0.8 threshold but far above null ({family4.get('null_simulated_mean', 0.12):.3f}). \"\n",
    "            f\"Z-score: {stability_z:.1f}. Directions are highly non-random \"\n",
    "            f\"but below the ambitious 0.8 target.\"\n",
    "        ),\n",
    "        \"statistical_values\": {\n",
    "            \"k3_cosine\": round(k3_cos, 6),\n",
    "            \"k5_cosine\": round(k5_cos, 6),\n",
    "            \"mean_cosine\": round(overall_stability, 6),\n",
    "            \"threshold\": sc3_threshold,\n",
    "            \"above_threshold\": sc3_met,\n",
    "            \"z_score_vs_null\": round(stability_z, 4),\n",
    "        },\n",
    "        \"verdict\": \"PARTIALLY_MET\" if overall_stability > 0.6 else \"NOT_MET\",\n",
    "    })\n",
    "    log.info(\"SC3: stability=%.3f, threshold=0.8, met=%s\", overall_stability, sc3_met)\n",
    "\n",
    "    # --- SUCCESS CRITERION 4 ---\n",
    "    # 'Clear Pareto frontier with sweet spot at K=4-6'\n",
    "    k_sweep_flat = family3.get(\"flatness_classification\") == \"flat\"\n",
    "\n",
    "    criteria.append({\n",
    "        \"id\": \"SC4\",\n",
    "        \"description\": \"Clear Pareto frontier with sweet spot at K=4-6\",\n",
    "        \"evidence_summary\": (\n",
    "            f\"K-sweep is perfectly flat (range={family3.get('range', 0):.4f}). \"\n",
    "            f\"No accuracy-interpretability tradeoff exists. K=2 weakly \"\n",
    "            f\"Pareto-dominates all higher K. The 'sweet spot' is K=2, \"\n",
    "            f\"not K=4-6 as predicted.\"\n",
    "        ),\n",
    "        \"statistical_values\": {\n",
    "            \"k_sweep_range\": family3.get(\"range\", 0),\n",
    "            \"k_sweep_flat\": k_sweep_flat,\n",
    "            \"k2_dominates\": family5.get(\"k2_dominates_all_dots\", False),\n",
    "        },\n",
    "        \"verdict\": \"NOT_MET\",\n",
    "    })\n",
    "    log.info(\"SC4: NOT_MET (flat sweep, no sweet spot)\")\n",
    "\n",
    "    # --- DISCONFIRMATION CRITERION 1 ---\n",
    "    # '>3% accuracy loss on most datasets'\n",
    "    figs_aa_acc = method_accs.get(\"figs_axis_aligned\", 0)\n",
    "    gap_vs_aa = figs_aa_acc - dots_best_acc\n",
    "    gap_vs_oblique = figs_oblique_acc - dots_best_acc\n",
    "\n",
    "    dc1_triggered = gap_vs_oblique > 0.03\n",
    "\n",
    "    criteria.append({\n",
    "        \"id\": \"DC1\",\n",
    "        \"description\": \">3% accuracy loss vs baselines on most datasets\",\n",
    "        \"evidence_summary\": (\n",
    "            f\"DOTS vs FIGS-AA: gap={gap_vs_aa*100:.1f}%. \"\n",
    "            f\"DOTS vs FIGS-oblique: gap={gap_vs_oblique*100:.1f}%. \"\n",
    "            f\"The 5% gap vs oblique exceeds the 3% disconfirmation threshold. \"\n",
    "            f\"However, DOTS vs axis-aligned gap is only 2.5% (within threshold). \"\n",
    "            f\"Single dataset prevents assessing 'most datasets'.\"\n",
    "        ),\n",
    "        \"statistical_values\": {\n",
    "            \"gap_vs_figs_aa\": round(gap_vs_aa, 6),\n",
    "            \"gap_vs_figs_oblique\": round(gap_vs_oblique, 6),\n",
    "            \"exceeds_3pct_aa\": gap_vs_aa > 0.03,\n",
    "            \"exceeds_3pct_oblique\": gap_vs_oblique > 0.03,\n",
    "        },\n",
    "        \"verdict\": \"PARTIALLY_TRIGGERED\",\n",
    "    })\n",
    "    log.info(\"DC1: gap_oblique=%.3f, triggered=%s\", gap_vs_oblique, dc1_triggered)\n",
    "\n",
    "    # --- DISCONFIRMATION CRITERION 2 ---\n",
    "    # 'Unstable dictionary directions'\n",
    "    dc2_triggered = overall_stability < 0.5\n",
    "\n",
    "    criteria.append({\n",
    "        \"id\": \"DC2\",\n",
    "        \"description\": \"Unstable dictionary directions (low cosine similarity)\",\n",
    "        \"evidence_summary\": (\n",
    "            f\"Mean stability: {overall_stability:.3f} (z={stability_z:.1f} vs null). \"\n",
    "            f\"Directions are highly stable \u2014 well above random chance. \"\n",
    "            f\"Disconfirmation NOT triggered.\"\n",
    "        ),\n",
    "        \"statistical_values\": {\n",
    "            \"mean_stability\": round(overall_stability, 6),\n",
    "            \"z_score\": round(stability_z, 4),\n",
    "            \"below_0_5\": dc2_triggered,\n",
    "        },\n",
    "        \"verdict\": \"NOT_TRIGGERED\",\n",
    "    })\n",
    "    log.info(\"DC2: NOT_TRIGGERED (stability=%.3f)\", overall_stability)\n",
    "\n",
    "    # --- Overall Verdict ---\n",
    "    verdicts = [c[\"verdict\"] for c in criteria]\n",
    "    n_met = sum(1 for v in verdicts if v in (\"MET\", \"MET_ON_THIS_DATASET\"))\n",
    "    n_partial = sum(1 for v in verdicts if \"PARTIAL\" in v)\n",
    "    n_not_met = sum(1 for v in verdicts if v == \"NOT_MET\")\n",
    "    n_triggered = sum(1 for v in verdicts if \"TRIGGERED\" in v and \"NOT_TRIGGERED\" not in v)\n",
    "\n",
    "    if n_met >= 3 and n_triggered == 0:\n",
    "        overall = \"SUPPORTED\"\n",
    "    elif n_met >= 2 or (n_met >= 1 and n_partial >= 1):\n",
    "        overall = \"PARTIALLY_SUPPORTED\"\n",
    "    elif n_not_met >= 3 or n_triggered >= 2:\n",
    "        overall = \"REFUTED\"\n",
    "    elif n_triggered >= 1:\n",
    "        overall = \"PARTIALLY_REFUTED\"\n",
    "    else:\n",
    "        overall = \"INCONCLUSIVE\"\n",
    "\n",
    "    justification = (\n",
    "        \"The DOTS hypothesis receives mixed support. SC2 (fewer directions) \"\n",
    "        \"is clearly met: DOTS uses 2-6 directions vs 44 for unconstrained FIGS. \"\n",
    "        \"SC3 (stability) is partially met: cosine similarity of 0.75 is far above \"\n",
    "        \"random (z>6, p<1e-10) but below the 0.8 threshold. \"\n",
    "        \"SC1 (within 2% of RO-FIGS) is NOT met: the 5% gap exceeds the threshold. \"\n",
    "        \"SC4 (Pareto sweet spot) is NOT met: the K-sweep is perfectly flat, \"\n",
    "        \"meaning K=2 dominates rather than K=4-6. \"\n",
    "        \"DC1 is partially triggered (5% gap vs oblique FIGS). \"\n",
    "        \"DC2 is NOT triggered (directions are highly stable). \"\n",
    "        \"The flat K-sweep is scientifically interesting \u2014 it suggests the \"\n",
    "        \"dictionary constraint is 'free' in terms of accuracy \u2014 but the overall \"\n",
    "        \"accuracy gap vs stronger baselines limits the practical claim. \"\n",
    "        \"Single-dataset evaluation prevents generalizability claims.\"\n",
    "    )\n",
    "\n",
    "    limitations = [\n",
    "        \"Single dataset (OpenML-797) \u2014 cannot assess generalizability\",\n",
    "        \"Per-example predictions only available for dots_K5 vs figs_axis_aligned\",\n",
    "        \"Small test set (n=40) limits statistical power for McNemar tests\",\n",
    "        \"DOTS predicts majority class for all examples (predict_method='1' for all)\",\n",
    "        \"No AUROC per-example data available for bootstrap\",\n",
    "        \"K-sweep flatness may be an artifact of the majority-class prediction pattern\",\n",
    "    ]\n",
    "\n",
    "    future_work = [\n",
    "        \"Evaluate on multiple diverse datasets to test generalizability\",\n",
    "        \"Investigate why all DOTS K values produce identical predictions\",\n",
    "        \"Compare against stronger oblique tree baselines (e.g., CART-oblique)\",\n",
    "        \"Test with larger sample sizes to improve statistical power\",\n",
    "        \"Store per-example predictions for all methods to enable full McNemar analysis\",\n",
    "        \"Investigate whether PCA initialization dominates dictionary learning\",\n",
    "    ]\n",
    "\n",
    "    result = {\n",
    "        \"criteria\": criteria,\n",
    "        \"overall_verdict\": overall,\n",
    "        \"verdict_justification\": justification,\n",
    "        \"summary_counts\": {\n",
    "            \"met\": n_met,\n",
    "            \"partially_met\": n_partial,\n",
    "            \"not_met\": n_not_met,\n",
    "            \"triggered\": n_triggered,\n",
    "        },\n",
    "        \"limitations\": limitations,\n",
    "        \"future_work\": future_work,\n",
    "    }\n",
    "\n",
    "    log.info(\"Overall verdict: %s\", overall)\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "d025cab2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Aggregate Metrics"
   ],
   "id": "53bf9b63"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 8: Aggregate Metrics\n",
    "# ===================================================================\n",
    "def compute_aggregate_metrics(test_exs, train_exs, dots_full, family1, family2,\n",
    "                               family3, family4, family5, family6):\n",
    "    \"\"\"Compute flat numeric metrics for metrics_agg (schema requirement).\"\"\"\n",
    "    log.info(\"=== Computing Aggregate Metrics ===\")\n",
    "\n",
    "    method_accs = dots_full.get(\"method_accuracies\", {})\n",
    "    n_test = len(test_exs)\n",
    "    n_total = len(test_exs) + len(train_exs)\n",
    "\n",
    "    # Test accuracy\n",
    "    true_labels = np.array([int(e[\"output\"]) for e in test_exs]) if test_exs else np.array([])\n",
    "    pred_method = np.array([int(e[\"predict_method\"]) for e in test_exs]) if test_exs else np.array([])\n",
    "    pred_baseline = np.array([int(e[\"predict_baseline\"]) for e in test_exs]) if test_exs else np.array([])\n",
    "\n",
    "    method_acc = float(np.mean(pred_method == true_labels)) if n_test > 0 else 0.0\n",
    "    baseline_acc = float(np.mean(pred_baseline == true_labels)) if n_test > 0 else 0.0\n",
    "\n",
    "    # Core metrics\n",
    "    metrics = {\n",
    "        \"n_test_examples\": n_test,\n",
    "        \"n_total_examples\": n_total,\n",
    "        \"dots_k5_test_accuracy\": round(method_acc, 6),\n",
    "        \"figs_aa_test_accuracy\": round(baseline_acc, 6),\n",
    "        \"accuracy_gap_method_vs_baseline\": round(method_acc - baseline_acc, 6),\n",
    "    }\n",
    "\n",
    "    # Method accuracies from aggregate\n",
    "    for name, acc in method_accs.items():\n",
    "        safe_name = name.replace(\" \", \"_\")\n",
    "        metrics[f\"acc_{safe_name}\"] = round(acc, 6)\n",
    "\n",
    "    # Family 2: McNemar\n",
    "    if family2:\n",
    "        metrics[\"mcnemar_p_value\"] = round(family2[0].get(\"p_value\", 1.0), 6)\n",
    "        metrics[\"mcnemar_odds_ratio\"] = round(\n",
    "            min(family2[0].get(\"odds_ratio\", 1.0), 999.0), 6\n",
    "        )\n",
    "\n",
    "    # Family 3: K-sweep\n",
    "    if family3:\n",
    "        metrics[\"k_sweep_range\"] = round(family3.get(\"range\", 0), 6)\n",
    "        metrics[\"k_sweep_cv\"] = round(family3.get(\"cv\", 0), 8)\n",
    "        metrics[\"k_sweep_spearman_rho\"] = round(family3.get(\"spearman_rho\", 0), 6)\n",
    "\n",
    "    # Family 4: Stability\n",
    "    if family4:\n",
    "        metrics[\"stability_mean_cosine\"] = round(family4.get(\"observed_mean\", 0), 6)\n",
    "        metrics[\"stability_null_mean\"] = round(family4.get(\"null_simulated_mean\", 0), 6)\n",
    "        metrics[\"stability_z_score\"] = round(family4.get(\"z_score\", 0), 4)\n",
    "        metrics[\"stability_uplift_ratio\"] = round(family4.get(\"stability_uplift_ratio\", 0), 4)\n",
    "\n",
    "    # Family 5: Pareto\n",
    "    if family5:\n",
    "        metrics[\"k2_dominates_all\"] = 1 if family5.get(\"k2_dominates_all_dots\") else 0\n",
    "\n",
    "    # Family 6: Verdict\n",
    "    verdict_map = {\n",
    "        \"SUPPORTED\": 5, \"PARTIALLY_SUPPORTED\": 4, \"INCONCLUSIVE\": 3,\n",
    "        \"PARTIALLY_REFUTED\": 2, \"REFUTED\": 1,\n",
    "    }\n",
    "    if family6:\n",
    "        v = family6.get(\"overall_verdict\", \"INCONCLUSIVE\")\n",
    "        metrics[\"verdict_score\"] = verdict_map.get(v, 3)\n",
    "        sc = family6.get(\"summary_counts\", {})\n",
    "        metrics[\"criteria_met\"] = sc.get(\"met\", 0)\n",
    "        metrics[\"criteria_partial\"] = sc.get(\"partially_met\", 0)\n",
    "        metrics[\"criteria_not_met\"] = sc.get(\"not_met\", 0)\n",
    "        metrics[\"criteria_triggered\"] = sc.get(\"triggered\", 0)\n",
    "\n",
    "    # Bootstrap CIs\n",
    "    if family1:\n",
    "        if \"dots_K5\" in family1:\n",
    "            ci = family1[\"dots_K5\"][\"accuracy\"]\n",
    "            metrics[\"dots_k5_ci_lower\"] = ci.get(\"ci_lower\", 0)\n",
    "            metrics[\"dots_k5_ci_upper\"] = ci.get(\"ci_upper\", 0)\n",
    "            metrics[\"dots_k5_boot_se\"] = ci.get(\"se\", 0)\n",
    "        if \"figs_axis_aligned\" in family1:\n",
    "            ci = family1[\"figs_axis_aligned\"][\"accuracy\"]\n",
    "            metrics[\"figs_aa_ci_lower\"] = ci.get(\"ci_lower\", 0)\n",
    "            metrics[\"figs_aa_ci_upper\"] = ci.get(\"ci_upper\", 0)\n",
    "\n",
    "    # Sanitize NaN/Inf values (schema requires numbers, not NaN)\n",
    "    for k in list(metrics.keys()):\n",
    "        v = metrics[k]\n",
    "        if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "            log.warning(\"Replacing NaN/Inf in metrics_agg[%s] with 0.0\", k)\n",
    "            metrics[k] = 0.0\n",
    "\n",
    "    log.info(\"Aggregate metrics: %d entries\", len(metrics))\n",
    "    return metrics"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "fff001be"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Build Output"
   ],
   "id": "0f851a77"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Section 9: Build Output\n",
    "# ===================================================================\n",
    "def build_output(examples, test_exs, metrics_agg, family1, family2, family3,\n",
    "                 family4, family5, family6):\n",
    "    \"\"\"Build output conforming to exp_eval_sol_out schema.\"\"\"\n",
    "    log.info(\"=== Building Output ===\")\n",
    "\n",
    "    # Per-example eval metrics\n",
    "    output_examples = []\n",
    "    for ex in examples:\n",
    "        true_label = int(ex[\"output\"])\n",
    "        pred_m = int(ex[\"predict_method\"])\n",
    "        pred_b = int(ex[\"predict_baseline\"])\n",
    "\n",
    "        out = {\n",
    "            \"input\": ex[\"input\"],\n",
    "            \"output\": ex[\"output\"],\n",
    "            \"context\": {\n",
    "                k: v for k, v in ex.get(\"context\", {}).items()\n",
    "                if k != \"dots_full_results\"\n",
    "            },\n",
    "            \"dataset\": ex[\"dataset\"],\n",
    "            \"split\": ex[\"split\"],\n",
    "            \"predict_baseline\": ex[\"predict_baseline\"],\n",
    "            \"predict_method\": ex[\"predict_method\"],\n",
    "            \"method\": ex[\"method\"],\n",
    "            \"eval_correct_method\": 1 if pred_m == true_label else 0,\n",
    "            \"eval_correct_baseline\": 1 if pred_b == true_label else 0,\n",
    "            \"eval_agree\": 1 if pred_m == pred_b else 0,\n",
    "        }\n",
    "        output_examples.append(out)\n",
    "\n",
    "    # Store full analysis in first example's context\n",
    "    if output_examples:\n",
    "        output_examples[0][\"context\"][\"evaluation_results\"] = {\n",
    "            \"family1_bootstrap_ci\": family1,\n",
    "            \"family2_mcnemar\": family2,\n",
    "            \"family3_k_sweep\": family3,\n",
    "            \"family4_stability\": family4,\n",
    "            \"family5_pareto\": family5,\n",
    "            \"family6_verdict\": family6,\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        \"metrics_agg\": metrics_agg,\n",
    "        \"examples\": output_examples,\n",
    "    }\n",
    "\n",
    "    log.info(\"Output: %d examples, %d agg metrics\", len(output_examples), len(metrics_agg))\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "035f98e8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation",
    "",
    "Execute the full 6-family statistical evaluation pipeline."
   ],
   "id": "5eefd1f2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Run the full evaluation pipeline\n",
    "# ===================================================================\n",
    "t0 = time.time()\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "log.info(\"=\" * 60)\n",
    "log.info(\"DOTS Statistical Evaluation \u2014 Starting\")\n",
    "log.info(\"MAX_EXAMPLES=%s, BOOTSTRAP_B=%d, NULL_SIM_N=%d\",\n",
    "         MAX_EXAMPLES, BOOTSTRAP_B, NULL_SIM_N)\n",
    "log.info(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "examples, test_exs, train_exs, dots_full = load_experiment(MAX_EXAMPLES)\n",
    "\n",
    "# Sanity checks\n",
    "assert len(examples) > 0, \"No examples loaded\"\n",
    "assert dots_full is not None, \"dots_full_results not found\"\n",
    "log.info(\"Sanity: %d examples, %d test, %d train\",\n",
    "         len(examples), len(test_exs), len(train_exs))\n",
    "\n",
    "# Check for majority-class prediction pattern\n",
    "pred_vals = set(e[\"predict_method\"] for e in examples)\n",
    "if len(pred_vals) == 1:\n",
    "    log.warning(\n",
    "        \"CRITICAL: predict_method is constant ('%s') \u2014 \"\n",
    "        \"DOTS may be a majority-class predictor!\",\n",
    "        list(pred_vals)[0],\n",
    "    )\n",
    "\n",
    "# Compute all families\n",
    "family1 = compute_family1(test_exs, dots_full, rng)\n",
    "family2 = compute_family2(test_exs, dots_full)\n",
    "family3 = compute_family3(test_exs, dots_full)\n",
    "family4 = compute_family4(dots_full, rng)\n",
    "family5 = compute_family5(dots_full, family2)\n",
    "family6 = compute_family6(dots_full, family1, family2, family3, family4, family5)\n",
    "\n",
    "# Aggregate metrics\n",
    "metrics_agg = compute_aggregate_metrics(\n",
    "    test_exs, train_exs, dots_full, family1, family2, family3,\n",
    "    family4, family5, family6,\n",
    ")\n",
    "\n",
    "# Build output\n",
    "output = build_output(\n",
    "    examples, test_exs, metrics_agg,\n",
    "    family1, family2, family3, family4, family5, family6,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "log.info(\"=\" * 60)\n",
    "log.info(\"Evaluation complete in %.1fs\", elapsed)\n",
    "log.info(\"Verdict: %s\", family6.get(\"overall_verdict\", \"UNKNOWN\"))\n",
    "log.info(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e826acf0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ],
   "id": "fdd3ce4a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# Print summary table\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOTS STATISTICAL EVALUATION \u2014 SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Aggregate Metrics ---\")\n",
    "for k, v in sorted(metrics_agg.items()):\n",
    "    print(f\"  {k:40s}: {v}\")\n",
    "\n",
    "print(\"\\n--- Family 6: Hypothesis Verdict ---\")\n",
    "print(f\"  Overall: {family6['overall_verdict']}\")\n",
    "for c in family6[\"criteria\"]:\n",
    "    print(f\"  {c['id']:4s} | {c['verdict']:20s} | {c['description']}\")\n",
    "\n",
    "print(\"\\n--- Key Findings ---\")\n",
    "print(f\"  DOTS K5 accuracy:    {metrics_agg.get('dots_k5_test_accuracy', 'N/A')}\")\n",
    "print(f\"  FIGS-AA accuracy:    {metrics_agg.get('figs_aa_test_accuracy', 'N/A')}\")\n",
    "print(f\"  McNemar p-value:     {metrics_agg.get('mcnemar_p_value', 'N/A')}\")\n",
    "print(f\"  K-sweep range:       {metrics_agg.get('k_sweep_range', 'N/A')}\")\n",
    "print(f\"  Stability z-score:   {metrics_agg.get('stability_z_score', 'N/A')}\")\n",
    "print(f\"  Stability uplift:    {metrics_agg.get('stability_uplift_ratio', 'N/A')}x\")\n",
    "print(\"=\" * 70)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "840d0b56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization",
    "",
    "4-panel summary of the DOTS evaluation results."
   ],
   "id": "cae3a74f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"DOTS Statistical Evaluation Summary\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# --- Panel 1: Method Accuracies with CIs ---\n",
    "ax = axes[0, 0]\n",
    "methods = []\n",
    "points = []\n",
    "lowers = []\n",
    "uppers = []\n",
    "for name, data in sorted(family1.items()):\n",
    "    methods.append(name)\n",
    "    acc = data[\"accuracy\"]\n",
    "    points.append(acc[\"point\"])\n",
    "    lowers.append(acc[\"point\"] - acc[\"ci_lower\"])\n",
    "    uppers.append(acc[\"ci_upper\"] - acc[\"point\"])\n",
    "\n",
    "y_pos = range(len(methods))\n",
    "ax.barh(y_pos, points, xerr=[lowers, uppers], capsize=3, color=\"steelblue\", alpha=0.7)\n",
    "ax.set_yticks(list(y_pos))\n",
    "ax.set_yticklabels(methods, fontsize=7)\n",
    "ax.set_xlabel(\"Test Accuracy\")\n",
    "ax.set_title(\"Method Accuracies with 95% CIs\")\n",
    "ax.axvline(x=0.725, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"DOTS K5\")\n",
    "ax.set_xlim(0.4, 1.0)\n",
    "\n",
    "# --- Panel 2: K-Sweep Flatness ---\n",
    "ax = axes[0, 1]\n",
    "k_vals = family3.get(\"k_values\", [])\n",
    "accs = family3.get(\"accuracies\", [])\n",
    "if k_vals and accs:\n",
    "    ax.plot(k_vals, accs, \"bo-\", markersize=8, linewidth=2)\n",
    "    ax.fill_between(k_vals,\n",
    "                    [a - 0.07 for a in accs],\n",
    "                    [a + 0.07 for a in accs],\n",
    "                    alpha=0.2, color=\"blue\")\n",
    "    ax.set_xlabel(\"K (dictionary size)\")\n",
    "    ax.set_ylabel(\"Test Accuracy\")\n",
    "    classification = family3.get(\"flatness_classification\", \"unknown\").upper()\n",
    "    sweep_range = family3.get(\"range\", 0)\n",
    "    ax.set_title(\"K-Sweep: \" + classification + \"\\n(range=\" + f\"{sweep_range:.4f})\")\n",
    "    ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "# --- Panel 3: Dictionary Stability vs Null ---\n",
    "ax = axes[1, 0]\n",
    "obs_mean = family4.get(\"observed_mean\", 0)\n",
    "null_mean = family4.get(\"null_simulated_mean\", 0)\n",
    "null_std = family4.get(\"null_simulated_std\", 0)\n",
    "z = family4.get(\"z_score\", 0)\n",
    "bars = ax.bar([\"Observed\", \"Null (random)\"], [obs_mean, null_mean],\n",
    "              color=[\"green\", \"gray\"], alpha=0.7)\n",
    "ax.errorbar([\"Observed\", \"Null (random)\"],\n",
    "            [obs_mean, null_mean],\n",
    "            yerr=[family4.get(\"observed_std\", 0), null_std],\n",
    "            fmt=\"none\", color=\"black\", capsize=5)\n",
    "ax.set_ylabel(\"Mean Cosine Similarity\")\n",
    "ax.set_title(\"Dictionary Stability vs Null\\n\" + f\"(z={z:.1f}, uplift={family4.get('stability_uplift_ratio', 0):.1f}x)\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# --- Panel 4: Hypothesis Verdict Scorecard ---\n",
    "ax = axes[1, 1]\n",
    "ax.axis(\"off\")\n",
    "verdict = family6.get(\"overall_verdict\", \"UNKNOWN\")\n",
    "criteria = family6.get(\"criteria\", [])\n",
    "\n",
    "verdict_colors = {\n",
    "    \"MET\": \"green\", \"MET_ON_THIS_DATASET\": \"green\",\n",
    "    \"PARTIALLY_MET\": \"orange\", \"NOT_MET\": \"red\",\n",
    "    \"PARTIALLY_TRIGGERED\": \"orange\", \"NOT_TRIGGERED\": \"green\",\n",
    "}\n",
    "\n",
    "y_start = 0.95\n",
    "ax.text(0.5, y_start, \"Hypothesis Verdict: \" + verdict,\n",
    "        fontsize=13, fontweight=\"bold\", ha=\"center\", va=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "        color=\"darkorange\" if \"PARTIAL\" in verdict else \"red\")\n",
    "\n",
    "for i, c in enumerate(criteria):\n",
    "    y = y_start - 0.12 - i * 0.1\n",
    "    color = verdict_colors.get(c[\"verdict\"], \"black\")\n",
    "    marker = {\"MET\": \"\\u2713\", \"MET_ON_THIS_DATASET\": \"\\u2713\",\n",
    "              \"NOT_MET\": \"\\u2717\", \"NOT_TRIGGERED\": \"\\u2713\",\n",
    "              \"PARTIALLY_MET\": \"~\", \"PARTIALLY_TRIGGERED\": \"~\"}.get(c[\"verdict\"], \"?\")\n",
    "    ax.text(0.05, y, f\"{marker} {c['id']}: {c['verdict']}\",\n",
    "            fontsize=9, color=color, transform=ax.transAxes, va=\"top\",\n",
    "            fontfamily=\"monospace\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dots_eval_summary.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: dots_eval_summary.png\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "4739dfd3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 \u2014 Full Run (Original Parameters)\n",
    "\n",
    "The demo above uses reduced parameters for fast execution (~30s).\n",
    "To reproduce the **original results** with full statistical power,\n",
    "uncomment the lines below and re-run the notebook from the top.\n",
    "\n",
    "> **Note:** The full run uses 10,000 bootstrap resamples and 10,000 null\n",
    "> simulations, which may take **5\u201310 minutes** depending on hardware."
   ],
   "id": "1e3b4074"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Uncomment the lines below to run with original parameters:\n",
    "# BOOTSTRAP_B = 10_000     # Original: 10,000 bootstrap resamples\n",
    "# NULL_SIM_N = 10_000      # Original: 10,000 null simulations\n",
    "# MAX_EXAMPLES = None      # Original: use all examples\n",
    "#\n",
    "# # Then re-run the evaluation pipeline:\n",
    "# rng = np.random.default_rng(RNG_SEED)\n",
    "# examples, test_exs, train_exs, dots_full = load_experiment(MAX_EXAMPLES)\n",
    "# family1 = compute_family1(test_exs, dots_full, rng)\n",
    "# family2 = compute_family2(test_exs, dots_full)\n",
    "# family3 = compute_family3(test_exs, dots_full)\n",
    "# family4 = compute_family4(dots_full, rng)\n",
    "# family5 = compute_family5(dots_full, family2)\n",
    "# family6 = compute_family6(dots_full, family1, family2, family3, family4, family5)\n",
    "# metrics_agg = compute_aggregate_metrics(\n",
    "#     test_exs, train_exs, dots_full, family1, family2, family3,\n",
    "#     family4, family5, family6,\n",
    "# )\n",
    "# output = build_output(\n",
    "#     examples, test_exs, metrics_agg,\n",
    "#     family1, family2, family3, family4, family5, family6,\n",
    "# )\n",
    "# print(f\"Full run complete. Verdict: {family6['overall_verdict']}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "f727c4b4"
  }
 ]
}