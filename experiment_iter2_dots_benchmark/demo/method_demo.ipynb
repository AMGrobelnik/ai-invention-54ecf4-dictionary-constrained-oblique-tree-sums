{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOTS: Dictionary-Constrained Oblique Tree Sums\n",
    "## Full Benchmark Experiment with K-Sweep and Stability Analysis\n",
    "\n",
    "This notebook implements **DOTS** (Dictionary-Constrained Oblique Tree Sums), a method that constrains oblique decision tree splits to use directions from a small learned dictionary. The approach balances the interpretability of axis-aligned trees with the accuracy of unconstrained oblique trees.\n",
    "\n",
    "**What this artifact does:**\n",
    "1. **FIGS-style** greedy competitive tree growth (axis-aligned + oblique baselines)\n",
    "2. **DOTS**: dictionary-constrained oblique splits with alternating optimization\n",
    "3. **Baselines**: RandomForest, DecisionTree, LogisticRegression\n",
    "4. **K-sweep** analysis for DOTS (varying dictionary size K)\n",
    "5. **Dictionary stability** via cross-validated Hungarian matching\n",
    "\n",
    "---\n",
    "\n",
    "**Part 1 (this notebook):** Quick demo on a small subset (~15 examples) with reduced parameters so it runs in seconds.\n",
    "\n",
    "**Part 2 (full version):** To run the full experiment with 200 examples, use `method.py` directly:\n",
    "```bash\n",
    ".venv/bin/python method.py                    # Run on full data (200 examples)\n",
    ".venv/bin/python method.py --max-examples 10  # Run on first 10 examples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging & Constants\n",
    "\n",
    "Set up DEBUG-level logging and define global constants used throughout the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Logging setup — DEBUG level as required\n",
    "# ---------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(\"dots\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "MIN_SAMPLES_LEAF = 5\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the demo dataset from GitHub (with local fallback for offline use). The data contains tabular binary classification examples from the OpenML-797 benchmark suite with 44 numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-54ecf4-dictionary-constrained-oblique-tree-sums/main/experiment_iter2_dots_benchmark/demo/demo_data.json\"\n",
    "LOCAL_FILE = \"demo_data.json\"\n",
    "\n",
    "import json, os\n",
    "\n",
    "def load_demo_data():\n",
    "    try:\n",
    "        import urllib.request\n",
    "        with urllib.request.urlopen(GITHUB_RAW_URL, timeout=10) as response:\n",
    "            data = json.loads(response.read().decode())\n",
    "            print(f\"Loaded data from GitHub URL\")\n",
    "            return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    if os.path.exists(LOCAL_FILE):\n",
    "        with open(LOCAL_FILE) as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"Loaded data from local file: {LOCAL_FILE}\")\n",
    "            return data\n",
    "    raise FileNotFoundError(\"Could not load data from GitHub or local file\")\n",
    "\n",
    "data = load_demo_data()\n",
    "raw_examples = data[\"examples\"]\n",
    "print(f\"Loaded {len(raw_examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Parsing & Train/Test Split\n",
    "\n",
    "Parse the loaded JSON examples into feature matrices and label arrays. If a train/test split already exists in the data, use it; otherwise create an 80/20 stratified split automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(raw_examples, max_examples=None):\n",
    "    \"\"\"Load JSON dataset, extract features/labels, separate train/test.\n",
    "\n",
    "    If all examples share the same split label, creates a stratified\n",
    "    80/20 train/test split automatically.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from pre-loaded examples\")\n",
    "\n",
    "    examples = list(raw_examples)\n",
    "    logger.debug(f\"Raw examples count: {len(examples)}\")\n",
    "\n",
    "    if max_examples is not None and max_examples < len(examples):\n",
    "        logger.info(f\"Limiting to first {max_examples} examples\")\n",
    "        examples = examples[:max_examples]\n",
    "\n",
    "    feature_names = None\n",
    "    all_X, all_y = [], []\n",
    "\n",
    "    for idx, ex in enumerate(examples):\n",
    "        try:\n",
    "            ctx = ex[\"context\"]\n",
    "            features_dict = ctx[\"features\"]\n",
    "            if feature_names is None:\n",
    "                feature_names = list(features_dict.keys())\n",
    "            x_row = [features_dict[fn] for fn in feature_names]\n",
    "            y_val = int(ex[\"output\"])\n",
    "            all_X.append(x_row)\n",
    "            all_y.append(y_val)\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Example {idx} missing key: {e}\")\n",
    "            raise\n",
    "\n",
    "    all_X = np.array(all_X, dtype=np.float64)\n",
    "    all_y = np.array(all_y, dtype=np.float64)\n",
    "    logger.debug(f\"Feature matrix shape: {all_X.shape}, labels shape: {all_y.shape}\")\n",
    "\n",
    "    # Check if data already has a meaningful train/test split\n",
    "    splits = [ex[\"split\"] for ex in examples]\n",
    "    unique_splits = set(splits)\n",
    "    logger.debug(f\"Unique split labels: {unique_splits}\")\n",
    "\n",
    "    if len(unique_splits) > 1 and \"train\" in unique_splits:\n",
    "        train_mask = np.array([s == \"train\" for s in splits])\n",
    "        test_mask = ~train_mask\n",
    "        logger.info(\"Using existing train/test split from data\")\n",
    "    else:\n",
    "        # AIDEV-NOTE: All examples share one split label → create 80/20 stratified split\n",
    "        logger.info(\"No train/test split found — creating stratified 80/20 split\")\n",
    "        from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        indices = np.arange(len(examples))\n",
    "        unique_classes, counts = np.unique(all_y, return_counts=True)\n",
    "        can_stratify = all(c >= 2 for c in counts) and len(examples) >= 5\n",
    "        logger.debug(f\"Class distribution: {dict(zip(unique_classes.astype(int), counts))}\")\n",
    "        logger.debug(f\"Can stratify: {can_stratify}\")\n",
    "\n",
    "        try:\n",
    "            tr_idx, te_idx = tts(\n",
    "                indices,\n",
    "                test_size=max(0.2, 1.0 / len(examples)),\n",
    "                stratify=all_y if can_stratify else None,\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Stratified split failed ({e}), falling back to random\")\n",
    "            n_test = max(1, len(examples) // 5)\n",
    "            rng = np.random.RandomState(RANDOM_SEED)\n",
    "            perm = rng.permutation(len(examples))\n",
    "            te_idx = perm[:n_test]\n",
    "            tr_idx = perm[n_test:]\n",
    "\n",
    "        train_mask = np.zeros(len(examples), dtype=bool)\n",
    "        train_mask[tr_idx] = True\n",
    "        test_mask = ~train_mask\n",
    "        for i in range(len(examples)):\n",
    "            examples[i][\"split\"] = \"train\" if train_mask[i] else \"test\"\n",
    "\n",
    "    train_indices = np.where(train_mask)[0].tolist()\n",
    "    test_indices = np.where(test_mask)[0].tolist()\n",
    "\n",
    "    X_train = all_X[train_mask]\n",
    "    y_train = all_y[train_mask]\n",
    "    X_test = all_X[test_mask]\n",
    "    y_test = all_y[test_mask]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Data loaded: {len(X_train)} train, {len(X_test)} test, \"\n",
    "        f\"{len(feature_names)} features, \"\n",
    "        f\"class balance={y_train.mean():.3f} positive\"\n",
    "    )\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(X_train) > 0, \"No training examples!\"\n",
    "    assert len(X_test) > 0, \"No test examples!\"\n",
    "    assert X_train.shape[1] == X_test.shape[1], \"Feature count mismatch!\"\n",
    "    assert not np.any(np.isnan(X_train)), \"NaN in training features!\"\n",
    "    assert not np.any(np.isnan(X_test)), \"NaN in test features!\"\n",
    "\n",
    "    return (\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        feature_names, examples, train_indices, test_indices,\n",
    "    )\n",
    "\n",
    "\n",
    "def standardize(X_train, X_test):\n",
    "    \"\"\"Z-score standardization fitted on train only.\"\"\"\n",
    "    mu = X_train.mean(axis=0)\n",
    "    sigma = X_train.std(axis=0) + 1e-8\n",
    "    X_tr_s = (X_train - mu) / sigma\n",
    "    X_te_s = (X_test - mu) / sigma\n",
    "    logger.debug(\n",
    "        f\"Standardized: train mean={X_tr_s.mean():.4f}, \"\n",
    "        f\"std={X_tr_s.std():.4f}\"\n",
    "    )\n",
    "    return X_tr_s, X_te_s, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Core Data Structures\n",
    "\n",
    "The tree ensemble is built from `LeafNode` and `SplitNode` objects. Each split projects data along a `direction_vector` and compares to a `threshold`. The `FIGSEnsemble` aggregates multiple trees with a logistic (sigmoid) link function for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 2: Core Data Structures\n",
    "# ===========================================================================\n",
    "class LeafNode:\n",
    "    __slots__ = (\"value\", \"n_samples\")\n",
    "\n",
    "    def __init__(self, value: float, n_samples: int):\n",
    "        self.value = value\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "\n",
    "class SplitNode:\n",
    "    __slots__ = (\n",
    "        \"direction_index\", \"direction_vector\", \"threshold\",\n",
    "        \"left\", \"right\", \"n_samples\",\n",
    "    )\n",
    "\n",
    "    def __init__(self, direction_index, direction_vector, threshold,\n",
    "                 left, right, n_samples):\n",
    "        self.direction_index = direction_index\n",
    "        self.direction_vector = direction_vector\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "\n",
    "def _traverse(node, x):\n",
    "    if isinstance(node, LeafNode):\n",
    "        return node.value\n",
    "    proj = np.dot(node.direction_vector, x)\n",
    "    return _traverse(node.left, x) if proj <= node.threshold else _traverse(node.right, x)\n",
    "\n",
    "\n",
    "class TreeModel:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([_traverse(self.root, X[i]) for i in range(len(X))])\n",
    "\n",
    "\n",
    "def _sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "class FIGSEnsemble:\n",
    "    def __init__(self, trees, intercept):\n",
    "        self.trees = trees\n",
    "        self.intercept = intercept\n",
    "\n",
    "    def predict_raw(self, X):\n",
    "        raw = np.full(len(X), self.intercept)\n",
    "        for tree in self.trees:\n",
    "            raw += tree.predict(X)\n",
    "        return raw\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return _sigmoid(self.predict_raw(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Split Finding & Greedy Growth\n",
    "\n",
    "The core FIGS algorithm greedily grows an ensemble of trees. At each step it competitively evaluates:\n",
    "- **Extending** an existing leaf into a subtree\n",
    "- **Starting** a new tree on the full residuals\n",
    "\n",
    "Split directions can be axis-aligned, unconstrained oblique (with coordinate descent refinement), or dictionary-constrained (DOTS mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 3: Split Finding & Greedy Growth\n",
    "# ===========================================================================\n",
    "def _best_threshold(projections, residuals):\n",
    "    \"\"\"Find best threshold for 1-D projections maximizing variance reduction.\"\"\"\n",
    "    n = len(projections)\n",
    "    if n < 2 * MIN_SAMPLES_LEAF:\n",
    "        return None, -np.inf\n",
    "\n",
    "    order = np.argsort(projections)\n",
    "    proj_sorted = projections[order]\n",
    "    res_sorted = residuals[order]\n",
    "\n",
    "    total_sum = res_sorted.sum()\n",
    "    total_sq = (res_sorted ** 2).sum()\n",
    "    total_var = total_sq - total_sum ** 2 / n\n",
    "\n",
    "    left_sum = 0.0\n",
    "    left_sq = 0.0\n",
    "    best_gain = -np.inf\n",
    "    best_threshold = None\n",
    "\n",
    "    for i in range(MIN_SAMPLES_LEAF - 1, n - MIN_SAMPLES_LEAF):\n",
    "        left_sum += res_sorted[i]\n",
    "        left_sq += res_sorted[i] ** 2\n",
    "        if proj_sorted[i] == proj_sorted[i + 1]:\n",
    "            continue\n",
    "        n_left = i + 1\n",
    "        n_right = n - n_left\n",
    "        right_sum = total_sum - left_sum\n",
    "        left_var = left_sq - left_sum ** 2 / n_left\n",
    "        right_var = (total_sq - left_sq) - right_sum ** 2 / n_right\n",
    "        gain = total_var - left_var - right_var\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_threshold = (proj_sorted[i] + proj_sorted[i + 1]) / 2.0\n",
    "\n",
    "    return best_threshold, best_gain\n",
    "\n",
    "\n",
    "def find_best_split(X, residuals, mode, dictionary=None):\n",
    "    \"\"\"Find the best split for data at a node.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    best = {\"direction_vector\": None, \"direction_index\": -1,\n",
    "            \"threshold\": None, \"gain\": -np.inf}\n",
    "\n",
    "    try:\n",
    "        if mode == \"axis_aligned\":\n",
    "            for j in range(n_features):\n",
    "                thr, gain = _best_threshold(X[:, j], residuals)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    dv = np.zeros(n_features)\n",
    "                    dv[j] = 1.0\n",
    "                    best = {\"direction_vector\": dv, \"direction_index\": j,\n",
    "                            \"threshold\": thr, \"gain\": gain}\n",
    "\n",
    "        elif mode == \"dots\":\n",
    "            K = len(dictionary)\n",
    "            for k in range(K):\n",
    "                proj = X @ dictionary[k]\n",
    "                thr, gain = _best_threshold(proj, residuals)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    best = {\"direction_vector\": dictionary[k].copy(),\n",
    "                            \"direction_index\": k, \"threshold\": thr, \"gain\": gain}\n",
    "\n",
    "        elif mode == \"oblique_unconstrained\":\n",
    "            candidates = []\n",
    "            # axis-aligned\n",
    "            for j in range(n_features):\n",
    "                dv = np.zeros(n_features)\n",
    "                dv[j] = 1.0\n",
    "                candidates.append(dv)\n",
    "            # random projections\n",
    "            rng = np.random.RandomState(RANDOM_SEED)\n",
    "            for _ in range(20):\n",
    "                rv = rng.randn(n_features)\n",
    "                rv /= np.linalg.norm(rv) + 1e-12\n",
    "                candidates.append(rv)\n",
    "            # PCA directions\n",
    "            if n_samples >= 3:\n",
    "                n_comp = min(3, n_features, n_samples)\n",
    "                pca = PCA(n_components=n_comp, random_state=RANDOM_SEED)\n",
    "                pca.fit(X)\n",
    "                for comp in pca.components_:\n",
    "                    candidates.append(comp / (np.linalg.norm(comp) + 1e-12))\n",
    "\n",
    "            for dv in candidates:\n",
    "                thr, gain = _best_threshold(X @ dv, residuals)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    best = {\"direction_vector\": dv.copy(), \"direction_index\": -1,\n",
    "                            \"threshold\": thr, \"gain\": gain}\n",
    "\n",
    "            # Coordinate descent refinement\n",
    "            if best[\"direction_vector\"] is not None:\n",
    "                cur_dv = best[\"direction_vector\"].copy()\n",
    "                for _ in range(5):\n",
    "                    improved = False\n",
    "                    for j in range(n_features):\n",
    "                        for delta in [-0.1, 0.1, -0.01, 0.01]:\n",
    "                            trial = cur_dv.copy()\n",
    "                            trial[j] += delta\n",
    "                            trial /= np.linalg.norm(trial) + 1e-12\n",
    "                            thr, gain = _best_threshold(X @ trial, residuals)\n",
    "                            if gain > best[\"gain\"]:\n",
    "                                best[\"direction_vector\"] = trial.copy()\n",
    "                                best[\"threshold\"] = thr\n",
    "                                best[\"gain\"] = gain\n",
    "                                cur_dv = trial.copy()\n",
    "                                improved = True\n",
    "                    if not improved:\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in find_best_split (mode={mode}): {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def _newton_leaf_value(residuals, proba, shrinkage=0.1):\n",
    "    \"\"\"Newton-Raphson leaf value for log-loss boosting.\"\"\"\n",
    "    hessian = proba * (1.0 - proba) + 1e-8\n",
    "    return shrinkage * residuals.sum() / hessian.sum()\n",
    "\n",
    "\n",
    "def grow_figs_ensemble(X_train, y_train, mode, max_splits=15,\n",
    "                       max_trees=5, dictionary=None, shrinkage=0.3):\n",
    "    \"\"\"Grow a FIGS-style competitive ensemble of trees.\"\"\"\n",
    "    n = len(y_train)\n",
    "    if n == 0:\n",
    "        logger.warning(\"Empty training set, returning empty ensemble\")\n",
    "        return FIGSEnsemble(trees=[], intercept=0.0)\n",
    "\n",
    "    p = np.clip(y_train.mean(), 0.01, 0.99)\n",
    "    intercept = float(np.log(p / (1.0 - p)))\n",
    "    logger.debug(\n",
    "        f\"grow_figs: mode={mode}, max_splits={max_splits}, max_trees={max_trees}, \"\n",
    "        f\"shrinkage={shrinkage}, n={n}, intercept={intercept:.4f}\"\n",
    "    )\n",
    "\n",
    "    trees = []\n",
    "    leaf_registry = []\n",
    "\n",
    "    for split_iter in range(max_splits):\n",
    "        preds = np.full(n, intercept)\n",
    "        for tree in trees:\n",
    "            preds += tree.predict(X_train)\n",
    "        proba = _sigmoid(preds)\n",
    "        residuals = y_train - proba\n",
    "\n",
    "        candidates = []\n",
    "\n",
    "        # Option A: extend existing leaf\n",
    "        for leaf_info in leaf_registry:\n",
    "            tree_idx, _, indices, _ = leaf_info\n",
    "            if len(indices) < 2 * MIN_SAMPLES_LEAF:\n",
    "                continue\n",
    "            split_info = find_best_split(\n",
    "                X=X_train[indices], residuals=residuals[indices],\n",
    "                mode=mode, dictionary=dictionary,\n",
    "            )\n",
    "            if split_info[\"gain\"] > 0:\n",
    "                candidates.append((split_info[\"gain\"], \"extend\", leaf_info, split_info))\n",
    "\n",
    "        # Option B: new tree\n",
    "        if len(trees) < max_trees:\n",
    "            all_idx = np.arange(n)\n",
    "            split_info = find_best_split(\n",
    "                X=X_train, residuals=residuals, mode=mode, dictionary=dictionary,\n",
    "            )\n",
    "            if split_info[\"gain\"] > 0:\n",
    "                candidates.append((split_info[\"gain\"], \"new_tree\", all_idx, split_info))\n",
    "\n",
    "        if not candidates:\n",
    "            logger.debug(f\"  No more valid splits at iteration {split_iter}\")\n",
    "            break\n",
    "\n",
    "        candidates.sort(key=lambda c: c[0], reverse=True)\n",
    "        _, action, info, split_info = candidates[0]\n",
    "        dv = split_info[\"direction_vector\"]\n",
    "        di = split_info[\"direction_index\"]\n",
    "        thr = split_info[\"threshold\"]\n",
    "\n",
    "        if action == \"new_tree\":\n",
    "            all_idx = info\n",
    "            proj = X_train[all_idx] @ dv\n",
    "            left_mask = proj <= thr\n",
    "            right_mask = ~left_mask\n",
    "            left_idx = all_idx[left_mask]\n",
    "            right_idx = all_idx[right_mask]\n",
    "            if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "                logger.debug(f\"  Empty child at split {split_iter}, stopping\")\n",
    "                break\n",
    "\n",
    "            left_leaf = LeafNode(\n",
    "                value=_newton_leaf_value(residuals[left_idx], proba[left_idx], shrinkage),\n",
    "                n_samples=len(left_idx),\n",
    "            )\n",
    "            right_leaf = LeafNode(\n",
    "                value=_newton_leaf_value(residuals[right_idx], proba[right_idx], shrinkage),\n",
    "                n_samples=len(right_idx),\n",
    "            )\n",
    "            root = SplitNode(di, dv, thr, left_leaf, right_leaf, len(all_idx))\n",
    "            new_tree = TreeModel(root)\n",
    "            trees.append(new_tree)\n",
    "            tree_idx = len(trees) - 1\n",
    "            leaf_registry.append((tree_idx, left_leaf, left_idx, (root, \"left\")))\n",
    "            leaf_registry.append((tree_idx, right_leaf, right_idx, (root, \"right\")))\n",
    "\n",
    "        elif action == \"extend\":\n",
    "            leaf_info = info\n",
    "            tree_idx, old_leaf, indices, (parent_node, side) = leaf_info\n",
    "            leaf_registry.remove(leaf_info)\n",
    "\n",
    "            proj = X_train[indices] @ dv\n",
    "            left_mask = proj <= thr\n",
    "            right_mask = ~left_mask\n",
    "            left_idx = indices[left_mask]\n",
    "            right_idx = indices[right_mask]\n",
    "            if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "                leaf_registry.append(leaf_info)\n",
    "                continue\n",
    "\n",
    "            left_leaf = LeafNode(\n",
    "                value=_newton_leaf_value(residuals[left_idx], proba[left_idx], shrinkage),\n",
    "                n_samples=len(left_idx),\n",
    "            )\n",
    "            right_leaf = LeafNode(\n",
    "                value=_newton_leaf_value(residuals[right_idx], proba[right_idx], shrinkage),\n",
    "                n_samples=len(right_idx),\n",
    "            )\n",
    "            new_node = SplitNode(di, dv, thr, left_leaf, right_leaf, len(indices))\n",
    "            if side == \"left\":\n",
    "                parent_node.left = new_node\n",
    "            else:\n",
    "                parent_node.right = new_node\n",
    "            leaf_registry.append((tree_idx, left_leaf, left_idx, (new_node, \"left\")))\n",
    "            leaf_registry.append((tree_idx, right_leaf, right_idx, (new_node, \"right\")))\n",
    "\n",
    "    # Refit leaf values\n",
    "    if trees:\n",
    "        preds = np.full(n, intercept)\n",
    "        for tree in trees:\n",
    "            preds += tree.predict(X_train)\n",
    "        proba_final = _sigmoid(preds)\n",
    "        residuals_final = y_train - proba_final\n",
    "        for leaf_info in leaf_registry:\n",
    "            _, leaf, indices, _ = leaf_info\n",
    "            if len(indices) > 0:\n",
    "                leaf.value = _newton_leaf_value(\n",
    "                    residuals_final[indices], proba_final[indices], shrinkage,\n",
    "                )\n",
    "\n",
    "    logger.debug(f\"  Built {len(trees)} trees with {sum(_count_splits(t.root) for t in trees)} total splits\")\n",
    "    return FIGSEnsemble(trees=trees, intercept=intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: DOTS Dictionary Initialization & Alternating Optimization\n",
    "\n",
    "DOTS constrains oblique splits to use directions from a learned dictionary of size K. The dictionary is initialized via PCA and then refined through alternating optimization:\n",
    "1. **Fix dictionary → grow trees** (greedy FIGS with dictionary-constrained splits)\n",
    "2. **Fix trees → optimize dictionary** (finite-difference gradient descent on log-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 4: DOTS Dictionary Init & Alternating Optimization\n",
    "# ===========================================================================\n",
    "def initialize_dictionary(X, K):\n",
    "    \"\"\"Initialize K dictionary directions using PCA.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_comp = min(K, n_features, n_samples)\n",
    "    logger.debug(f\"Initializing dictionary: K={K}, n_comp={n_comp}\")\n",
    "    pca = PCA(n_components=n_comp, random_state=RANDOM_SEED)\n",
    "    pca.fit(X)\n",
    "    dictionary = np.zeros((K, n_features))\n",
    "    for i in range(n_comp):\n",
    "        v = pca.components_[i]\n",
    "        dictionary[i] = v / (np.linalg.norm(v) + 1e-12)\n",
    "    rng = np.random.RandomState(RANDOM_SEED + 1)\n",
    "    for i in range(n_comp, K):\n",
    "        rv = rng.randn(n_features)\n",
    "        dictionary[i] = rv / (np.linalg.norm(rv) + 1e-12)\n",
    "    logger.debug(f\"Dictionary shape: {dictionary.shape}, norms: {np.linalg.norm(dictionary, axis=1)[:3]}...\")\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def _collect_splits(node):\n",
    "    if isinstance(node, LeafNode):\n",
    "        return []\n",
    "    result = [node]\n",
    "    result.extend(_collect_splits(node.left))\n",
    "    result.extend(_collect_splits(node.right))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _compute_log_loss(ensemble, X, y):\n",
    "    proba = np.clip(ensemble.predict_proba(X), 1e-7, 1 - 1e-7)\n",
    "    return float(-np.mean(y * np.log(proba) + (1 - y) * np.log(1 - proba)))\n",
    "\n",
    "\n",
    "def optimize_dictionary(ensemble, X_train, y_train, dictionary,\n",
    "                        n_steps=5, lr=0.005, epsilon=1e-4):\n",
    "    \"\"\"Optimize dictionary directions via finite-difference gradient descent.\"\"\"\n",
    "    K, n_features = dictionary.shape\n",
    "    dictionary = dictionary.copy()\n",
    "\n",
    "    used_indices = set()\n",
    "    for tree in ensemble.trees:\n",
    "        for node in _collect_splits(tree.root):\n",
    "            if node.direction_index >= 0:\n",
    "                used_indices.add(node.direction_index)\n",
    "    logger.debug(f\"  Dictionary optimization: used indices={used_indices}\")\n",
    "\n",
    "    for k in range(K):\n",
    "        if k not in used_indices:\n",
    "            continue\n",
    "        for step in range(n_steps):\n",
    "            _update_ensemble_directions(ensemble, dictionary)\n",
    "            grad = np.zeros(n_features)\n",
    "            for j in range(n_features):\n",
    "                dictionary[k, j] += epsilon\n",
    "                _update_ensemble_directions(ensemble, dictionary)\n",
    "                loss_plus = _compute_log_loss(ensemble, X_train, y_train)\n",
    "                dictionary[k, j] -= 2 * epsilon\n",
    "                _update_ensemble_directions(ensemble, dictionary)\n",
    "                loss_minus = _compute_log_loss(ensemble, X_train, y_train)\n",
    "                dictionary[k, j] += epsilon\n",
    "                grad[j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            dictionary[k] -= lr * grad\n",
    "            norm = np.linalg.norm(dictionary[k])\n",
    "            if norm > 1e-12:\n",
    "                dictionary[k] /= norm\n",
    "\n",
    "    _update_ensemble_directions(ensemble, dictionary)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def _update_ensemble_directions(ensemble, dictionary):\n",
    "    for tree in ensemble.trees:\n",
    "        for node in _collect_splits(tree.root):\n",
    "            idx = node.direction_index\n",
    "            if 0 <= idx < len(dictionary):\n",
    "                node.direction_vector = dictionary[idx].copy()\n",
    "\n",
    "\n",
    "def dots_full(X_train, y_train, K, max_splits=15, max_trees=5,\n",
    "              n_alternation_rounds=3):\n",
    "    \"\"\"Run full DOTS algorithm with alternating optimization.\"\"\"\n",
    "    logger.debug(f\"dots_full: K={K}, max_splits={max_splits}, rounds={n_alternation_rounds}\")\n",
    "    dictionary = initialize_dictionary(X_train, K)\n",
    "    ensemble = None\n",
    "\n",
    "    for round_idx in range(n_alternation_rounds):\n",
    "        ensemble = grow_figs_ensemble(\n",
    "            X_train=X_train, y_train=y_train, mode=\"dots\",\n",
    "            max_splits=max_splits, max_trees=max_trees, dictionary=dictionary,\n",
    "        )\n",
    "        if round_idx < n_alternation_rounds - 1:\n",
    "            try:\n",
    "                loss_before = _compute_log_loss(ensemble, X_train, y_train)\n",
    "                dictionary = optimize_dictionary(\n",
    "                    ensemble=ensemble, X_train=X_train, y_train=y_train,\n",
    "                    dictionary=dictionary, n_steps=5, lr=0.005,\n",
    "                )\n",
    "                loss_after = _compute_log_loss(ensemble, X_train, y_train)\n",
    "                logger.debug(\n",
    "                    f\"  DOTS K={K} round {round_idx}: \"\n",
    "                    f\"loss {loss_before:.4f} -> {loss_after:.4f}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Dictionary optimization failed at round {round_idx}: {e}\")\n",
    "                # Fallback: keep current dictionary unchanged\n",
    "                break\n",
    "\n",
    "    return ensemble, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Baselines\n",
    "\n",
    "Three standard sklearn baselines for comparison: RandomForest (100 trees, depth 5), DecisionTree (depth 4), and LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 5: Baselines\n",
    "# ===========================================================================\n",
    "def run_baseline_rf(X_train, y_train, X_test):\n",
    "    logger.debug(\"Running RandomForest baseline\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_SEED)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_pred = rf.predict(X_train)\n",
    "    test_pred = rf.predict(X_test)\n",
    "    test_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    return train_pred, test_pred, test_proba\n",
    "\n",
    "\n",
    "def run_baseline_dt(X_train, y_train, X_test):\n",
    "    logger.debug(\"Running DecisionTree baseline\")\n",
    "    dt = DecisionTreeClassifier(max_depth=4, random_state=RANDOM_SEED)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_proba = dt.predict_proba(X_test)[:, 1]\n",
    "    return train_pred, test_pred, test_proba\n",
    "\n",
    "\n",
    "def run_baseline_lr(X_train, y_train, X_test):\n",
    "    logger.debug(\"Running LogisticRegression baseline\")\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
    "    lr.fit(X_train, y_train)\n",
    "    train_pred = lr.predict(X_train)\n",
    "    test_pred = lr.predict(X_test)\n",
    "    test_proba = lr.predict_proba(X_test)[:, 1]\n",
    "    return train_pred, test_pred, test_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation Utilities\n",
    "\n",
    "Metrics computation (accuracy, AUROC), tree complexity counting, dictionary direction naming, and cross-fold dictionary stability analysis using Hungarian matching of cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 6: Evaluation Utilities\n",
    "# ===========================================================================\n",
    "def _count_splits(node):\n",
    "    if isinstance(node, LeafNode):\n",
    "        return 0\n",
    "    return 1 + _count_splits(node.left) + _count_splits(node.right)\n",
    "\n",
    "\n",
    "def count_total_splits(ensemble):\n",
    "    return sum(_count_splits(t.root) for t in ensemble.trees)\n",
    "\n",
    "\n",
    "def _collect_direction_indices(node):\n",
    "    if isinstance(node, LeafNode):\n",
    "        return []\n",
    "    result = [node.direction_index]\n",
    "    result.extend(_collect_direction_indices(node.left))\n",
    "    result.extend(_collect_direction_indices(node.right))\n",
    "    return result\n",
    "\n",
    "\n",
    "def count_unique_directions(ensemble):\n",
    "    all_dirs = []\n",
    "    for tree in ensemble.trees:\n",
    "        all_dirs.extend(_collect_direction_indices(tree.root))\n",
    "    return len(set(all_dirs))\n",
    "\n",
    "\n",
    "def evaluate_ensemble(ensemble, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate a FIGSEnsemble and return metrics dict.\"\"\"\n",
    "    try:\n",
    "        train_pred = ensemble.predict(X_train)\n",
    "        test_pred = ensemble.predict(X_test)\n",
    "        train_proba = ensemble.predict_proba(X_train)\n",
    "        test_proba = ensemble.predict_proba(X_test)\n",
    "\n",
    "        result = {\n",
    "            \"train_accuracy\": float(accuracy_score(y_train, train_pred)),\n",
    "            \"test_accuracy\": float(accuracy_score(y_test, test_pred)),\n",
    "            \"train_predictions\": train_pred.tolist(),\n",
    "            \"test_predictions\": test_pred.tolist(),\n",
    "            \"test_probabilities\": test_proba.tolist(),\n",
    "        }\n",
    "        # AUROC needs both classes present\n",
    "        if len(np.unique(y_test)) > 1 and len(np.unique(test_proba)) > 1:\n",
    "            result[\"test_auroc\"] = float(roc_auc_score(y_test, test_proba))\n",
    "        else:\n",
    "            result[\"test_auroc\"] = 0.5\n",
    "            logger.warning(\"AUROC undefined (single class or constant proba), defaulting to 0.5\")\n",
    "\n",
    "        if len(np.unique(y_train)) > 1 and len(np.unique(train_proba)) > 1:\n",
    "            result[\"train_auroc\"] = float(roc_auc_score(y_train, train_proba))\n",
    "        else:\n",
    "            result[\"train_auroc\"] = 0.5\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "def compute_metrics(train_pred, test_pred, test_proba, y_train, y_test):\n",
    "    \"\"\"Compute metrics for sklearn baselines.\"\"\"\n",
    "    result = {\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, train_pred)),\n",
    "        \"test_accuracy\": float(accuracy_score(y_test, test_pred)),\n",
    "        \"train_predictions\": train_pred.tolist(),\n",
    "        \"test_predictions\": test_pred.tolist(),\n",
    "        \"test_probabilities\": test_proba.tolist(),\n",
    "    }\n",
    "    if len(np.unique(y_test)) > 1 and len(np.unique(test_proba)) > 1:\n",
    "        result[\"test_auroc\"] = float(roc_auc_score(y_test, test_proba))\n",
    "    else:\n",
    "        result[\"test_auroc\"] = 0.5\n",
    "    return result\n",
    "\n",
    "\n",
    "def name_directions(dictionary, feature_names, top_k=3):\n",
    "    names = []\n",
    "    for k in range(len(dictionary)):\n",
    "        w = dictionary[k]\n",
    "        abs_w = np.abs(w)\n",
    "        top_idx = np.argsort(abs_w)[-top_k:][::-1]\n",
    "        parts = []\n",
    "        for idx in top_idx:\n",
    "            if abs_w[idx] > 0.05:\n",
    "                sign = \"+\" if w[idx] > 0 else \"-\"\n",
    "                parts.append(f\"{sign}{abs_w[idx]:.2f}*{feature_names[idx]}\")\n",
    "        name = f\"Concept_{k+1}: {' '.join(parts)}\" if parts else f\"Concept_{k+1}: uniform\"\n",
    "        names.append(name)\n",
    "    return names\n",
    "\n",
    "\n",
    "def compute_dictionary_stability(fold_dictionaries):\n",
    "    \"\"\"Compute pairwise cosine similarity between fold dictionaries.\"\"\"\n",
    "    pairwise_sims = []\n",
    "    try:\n",
    "        for i in range(len(fold_dictionaries)):\n",
    "            for j in range(i + 1, len(fold_dictionaries)):\n",
    "                cos_matrix = np.abs(fold_dictionaries[i] @ fold_dictionaries[j].T)\n",
    "                row_ind, col_ind = linear_sum_assignment(-cos_matrix)\n",
    "                matched = cos_matrix[row_ind, col_ind]\n",
    "                pairwise_sims.append(float(np.mean(matched)))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Stability computation failed: {e}\")\n",
    "        # Greedy fallback\n",
    "        for i in range(len(fold_dictionaries)):\n",
    "            for j in range(i + 1, len(fold_dictionaries)):\n",
    "                d_i, d_j = fold_dictionaries[i], fold_dictionaries[j]\n",
    "                cos_matrix = np.abs(d_i @ d_j.T)\n",
    "                sims = []\n",
    "                used = set()\n",
    "                for row in range(len(d_i)):\n",
    "                    best_col = -1\n",
    "                    best_sim = -1\n",
    "                    for col in range(len(d_j)):\n",
    "                        if col not in used and cos_matrix[row, col] > best_sim:\n",
    "                            best_sim = cos_matrix[row, col]\n",
    "                            best_col = col\n",
    "                    if best_col >= 0:\n",
    "                        used.add(best_col)\n",
    "                        sims.append(best_sim)\n",
    "                pairwise_sims.append(float(np.mean(sims)) if sims else 0.0)\n",
    "\n",
    "    return {\n",
    "        \"mean_cosine\": float(np.mean(pairwise_sims)) if pairwise_sims else 0.0,\n",
    "        \"pairwise\": pairwise_sims,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Full Experiment Pipeline (DEMO parameters)\n",
    "\n",
    "Runs the complete DOTS experiment with **reduced parameters** for fast demo execution:\n",
    "1. Load and standardize data\n",
    "2. FIGS axis-aligned and oblique baselines\n",
    "3. DOTS K-sweep (reduced set of K values)\n",
    "4. Sklearn baselines (RF, DT, LR)\n",
    "5. Dictionary stability analysis (skipped in demo due to small dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 7: Full Experiment Pipeline\n",
    "# ===========================================================================\n",
    "def run_full_experiment(raw_examples, max_examples=None):\n",
    "    \"\"\"Run the complete DOTS experiment.\"\"\"\n",
    "    t0 = time.time()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    # 1. Load data\n",
    "    (X_train, y_train, X_test, y_test,\n",
    "     feature_names, examples, train_indices, test_indices) = load_data(\n",
    "        raw_examples, max_examples=max_examples,\n",
    "    )\n",
    "\n",
    "    # 2. Standardize\n",
    "    X_train_s, X_test_s, mu, sigma = standardize(X_train, X_test)\n",
    "\n",
    "    results = {}\n",
    "    n_train = len(X_train_s)\n",
    "\n",
    "    # Adaptive hyperparameters based on dataset size\n",
    "    figs_max_splits = min(25, max(5, n_train // 6))\n",
    "    dots_max_splits = min(15, max(5, n_train // 10))\n",
    "    exp_max_trees = 3  # DEMO: reduced. Original: 5\n",
    "    logger.info(\n",
    "        f\"Hyperparameters: figs_splits={figs_max_splits}, \"\n",
    "        f\"dots_splits={dots_max_splits}, max_trees={exp_max_trees}\"\n",
    "    )\n",
    "\n",
    "    # 3a. Axis-aligned FIGS baseline\n",
    "    logger.info(\"Running axis-aligned FIGS baseline...\")\n",
    "    try:\n",
    "        figs_aa = grow_figs_ensemble(\n",
    "            X_train_s, y_train, mode=\"axis_aligned\",\n",
    "            max_splits=figs_max_splits, max_trees=exp_max_trees,\n",
    "            shrinkage=1.0,\n",
    "        )\n",
    "        results[\"figs_axis_aligned\"] = evaluate_ensemble(\n",
    "            figs_aa, X_train_s, y_train, X_test_s, y_test,\n",
    "        )\n",
    "        results[\"figs_axis_aligned\"][\"n_unique_directions\"] = count_unique_directions(figs_aa)\n",
    "        results[\"figs_axis_aligned\"][\"n_splits\"] = count_total_splits(figs_aa)\n",
    "        logger.info(f\"  FIGS AA: acc={results['figs_axis_aligned']['test_accuracy']:.4f}, \"\n",
    "                     f\"auroc={results['figs_axis_aligned']['test_auroc']:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FIGS axis-aligned failed: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    # 3b. Unconstrained oblique FIGS\n",
    "    logger.info(\"Running unconstrained oblique FIGS...\")\n",
    "    try:\n",
    "        figs_ob = grow_figs_ensemble(\n",
    "            X_train_s, y_train, mode=\"oblique_unconstrained\",\n",
    "            max_splits=figs_max_splits, max_trees=exp_max_trees,\n",
    "            shrinkage=1.0,\n",
    "        )\n",
    "        results[\"figs_oblique\"] = evaluate_ensemble(\n",
    "            figs_ob, X_train_s, y_train, X_test_s, y_test,\n",
    "        )\n",
    "        results[\"figs_oblique\"][\"n_unique_directions\"] = count_unique_directions(figs_ob)\n",
    "        results[\"figs_oblique\"][\"n_splits\"] = count_total_splits(figs_ob)\n",
    "        logger.info(f\"  FIGS oblique: acc={results['figs_oblique']['test_accuracy']:.4f}, \"\n",
    "                     f\"auroc={results['figs_oblique']['test_auroc']:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FIGS oblique failed: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    # 3c. DOTS K-sweep\n",
    "    K_values = [2, 3, 5]  # DEMO: reduced set. Original: [2, 3, 4, 5, 6, 8, 10]\n",
    "    for K in K_values:\n",
    "        logger.info(f\"Running DOTS K={K}...\")\n",
    "        try:\n",
    "            ens, dct = dots_full(\n",
    "                X_train_s, y_train, K=K,\n",
    "                max_splits=dots_max_splits, max_trees=exp_max_trees,\n",
    "                n_alternation_rounds=2,  # DEMO: reduced. Original: 3\n",
    "            )\n",
    "            key = f\"dots_K{K}\"\n",
    "            results[key] = evaluate_ensemble(ens, X_train_s, y_train, X_test_s, y_test)\n",
    "            results[key][\"K\"] = K\n",
    "            results[key][\"n_unique_directions\"] = K\n",
    "            results[key][\"n_splits\"] = count_total_splits(ens)\n",
    "            results[key][\"dictionary\"] = dct.tolist()\n",
    "            results[key][\"dictionary_feature_names\"] = feature_names\n",
    "            results[key][\"direction_names\"] = name_directions(dct, feature_names)\n",
    "            logger.info(f\"  DOTS K={K}: acc={results[key]['test_accuracy']:.4f}, \"\n",
    "                         f\"auroc={results[key]['test_auroc']:.4f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DOTS K={K} failed: {e}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "\n",
    "    # 3d. Sklearn baselines\n",
    "    logger.info(\"Running sklearn baselines...\")\n",
    "    try:\n",
    "        rf_tr, rf_te, rf_proba = run_baseline_rf(X_train_s, y_train, X_test_s)\n",
    "        results[\"random_forest\"] = compute_metrics(rf_tr, rf_te, rf_proba, y_train, y_test)\n",
    "        logger.info(f\"  RF: acc={results['random_forest']['test_accuracy']:.4f}\")\n",
    "\n",
    "        dt_tr, dt_te, dt_proba = run_baseline_dt(X_train_s, y_train, X_test_s)\n",
    "        results[\"decision_tree\"] = compute_metrics(dt_tr, dt_te, dt_proba, y_train, y_test)\n",
    "        logger.info(f\"  DT: acc={results['decision_tree']['test_accuracy']:.4f}\")\n",
    "\n",
    "        lr_tr, lr_te, lr_proba = run_baseline_lr(X_train_s, y_train, X_test_s)\n",
    "        results[\"logistic_regression\"] = compute_metrics(lr_tr, lr_te, lr_proba, y_train, y_test)\n",
    "        logger.info(f\"  LR: acc={results['logistic_regression']['test_accuracy']:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sklearn baselines failed: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    # 4. Dictionary stability (5-fold CV) — only if enough training data\n",
    "    if n_train >= 25:  # Original: 25 (same threshold, but demo dataset has <25 train samples)\n",
    "        logger.info(\"Running 5-fold CV stability analysis...\")\n",
    "        stability_results = {}\n",
    "        for K in [3, 5]:\n",
    "            fold_dicts = []\n",
    "            fold_accs = []\n",
    "            try:\n",
    "                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "                for fold_idx, (tr_idx, val_idx) in enumerate(skf.split(X_train_s, y_train)):\n",
    "                    ens_f, dict_f = dots_full(\n",
    "                        X_train_s[tr_idx], y_train[tr_idx], K=K,\n",
    "                        max_splits=max(5, dots_max_splits // 2),\n",
    "                        max_trees=3, n_alternation_rounds=2,\n",
    "                    )\n",
    "                    fold_dicts.append(dict_f)\n",
    "                    acc = float(np.mean(ens_f.predict(X_train_s[val_idx]) == y_train[val_idx]))\n",
    "                    fold_accs.append(acc)\n",
    "                    logger.info(f\"  Stability K={K} fold {fold_idx}: acc={acc:.4f}\")\n",
    "                stab = compute_dictionary_stability(fold_dicts)\n",
    "                stability_results[f\"K{K}\"] = {\n",
    "                    \"fold_accuracies\": fold_accs,\n",
    "                    \"mean_accuracy\": float(np.mean(fold_accs)),\n",
    "                    \"std_accuracy\": float(np.std(fold_accs)),\n",
    "                    \"mean_cosine_similarity\": stab[\"mean_cosine\"],\n",
    "                    \"pairwise_similarities\": stab[\"pairwise\"],\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Stability K={K} failed: {e}\")\n",
    "                stability_results[f\"K{K}\"] = {\n",
    "                    \"fold_accuracies\": fold_accs,\n",
    "                    \"mean_accuracy\": float(np.mean(fold_accs)) if fold_accs else 0.0,\n",
    "                    \"std_accuracy\": 0.0,\n",
    "                    \"mean_cosine_similarity\": 0.0,\n",
    "                    \"pairwise_similarities\": [],\n",
    "                }\n",
    "        results[\"stability_analysis\"] = stability_results\n",
    "    else:\n",
    "        logger.info(\"Skipping stability analysis (n_train < 25)\")\n",
    "        results[\"stability_analysis\"] = {}\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    logger.info(f\"Total experiment time: {elapsed:.1f}s\")\n",
    "\n",
    "    return results, examples, train_indices, test_indices, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Output Generation\n",
    "\n",
    "Generate output examples in the required schema format and produce a human-readable summary of all method accuracies, K-sweep results, and dictionary stability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Section 8: Output Generation\n",
    "# ===========================================================================\n",
    "def serialize_results_summary(results):\n",
    "    summary = {\n",
    "        \"method_accuracies\": {},\n",
    "        \"k_sweep_accuracies\": {},\n",
    "        \"stability_analysis\": results.get(\"stability_analysis\", {}),\n",
    "        \"pareto_frontier\": [],\n",
    "    }\n",
    "    for method_name, method_res in results.items():\n",
    "        if method_name == \"stability_analysis\":\n",
    "            continue\n",
    "        if isinstance(method_res, dict) and \"test_accuracy\" in method_res:\n",
    "            summary[\"method_accuracies\"][method_name] = method_res[\"test_accuracy\"]\n",
    "        if method_name.startswith(\"dots_K\"):\n",
    "            K = method_res.get(\"K\", 0)\n",
    "            summary[\"k_sweep_accuracies\"][f\"K={K}\"] = method_res[\"test_accuracy\"]\n",
    "            summary[\"pareto_frontier\"].append({\n",
    "                \"K\": K,\n",
    "                \"test_accuracy\": method_res[\"test_accuracy\"],\n",
    "                \"n_unique_directions\": K,\n",
    "                \"n_splits\": method_res.get(\"n_splits\", 0),\n",
    "                \"direction_names\": method_res.get(\"direction_names\", []),\n",
    "            })\n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_output(results, raw_examples, train_indices, test_indices):\n",
    "    \"\"\"Generate output examples in required schema format.\n",
    "\n",
    "    Schema: {examples: [{input, output, context, dataset, split,\n",
    "                         predict_baseline, predict_method, method}]}\n",
    "    \"\"\"\n",
    "    primary = \"dots_K5\"\n",
    "    baseline = \"figs_axis_aligned\"\n",
    "\n",
    "    # Sanity check: make sure primary and baseline exist\n",
    "    if primary not in results:\n",
    "        logger.warning(f\"Primary method '{primary}' not found, using figs_axis_aligned\")\n",
    "        primary = \"figs_axis_aligned\"\n",
    "    if baseline not in results:\n",
    "        logger.error(f\"Baseline '{baseline}' not found!\")\n",
    "        raise KeyError(f\"Baseline method '{baseline}' not in results\")\n",
    "\n",
    "    train_counter = 0\n",
    "    test_counter = 0\n",
    "    output_examples = []\n",
    "\n",
    "    for idx, ex in enumerate(raw_examples):\n",
    "        # Build context without dots_full_results for all except first\n",
    "        ctx = {}\n",
    "        for k, v in ex[\"context\"].items():\n",
    "            ctx[k] = v\n",
    "\n",
    "        out = {\n",
    "            \"input\": ex[\"input\"],\n",
    "            \"output\": ex[\"output\"],\n",
    "            \"context\": ctx,\n",
    "            \"dataset\": ex[\"dataset\"],\n",
    "            \"split\": ex[\"split\"],\n",
    "            \"method\": primary,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if ex[\"split\"] == \"train\":\n",
    "                out[\"predict_baseline\"] = str(\n",
    "                    results[baseline][\"train_predictions\"][train_counter]\n",
    "                )\n",
    "                out[\"predict_method\"] = str(\n",
    "                    results[primary][\"train_predictions\"][train_counter]\n",
    "                )\n",
    "                train_counter += 1\n",
    "            else:\n",
    "                out[\"predict_baseline\"] = str(\n",
    "                    results[baseline][\"test_predictions\"][test_counter]\n",
    "                )\n",
    "                out[\"predict_method\"] = str(\n",
    "                    results[primary][\"test_predictions\"][test_counter]\n",
    "                )\n",
    "                test_counter += 1\n",
    "        except (IndexError, KeyError) as e:\n",
    "            logger.error(f\"Error generating prediction for example {idx}: {e}\")\n",
    "            out[\"predict_baseline\"] = \"0\"\n",
    "            out[\"predict_method\"] = \"0\"\n",
    "\n",
    "        # Embed full results summary in first example only\n",
    "        if len(output_examples) == 0:\n",
    "            out[\"context\"][\"dots_full_results\"] = serialize_results_summary(results)\n",
    "\n",
    "        output_examples.append(out)\n",
    "\n",
    "    logger.info(f\"Generated {len(output_examples)} output examples \"\n",
    "                f\"(train_counter={train_counter}, test_counter={test_counter})\")\n",
    "    return output_examples\n",
    "\n",
    "\n",
    "def print_summary(results):\n",
    "    \"\"\"Print human-readable results summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\" * 65)\n",
    "    for method, res in results.items():\n",
    "        if method == \"stability_analysis\":\n",
    "            continue\n",
    "        if isinstance(res, dict) and \"test_accuracy\" in res:\n",
    "            auroc = res.get(\"test_auroc\", \"N/A\")\n",
    "            if isinstance(auroc, float):\n",
    "                auroc = f\"{auroc:.4f}\"\n",
    "            print(f\"  {method:30s}  acc={res['test_accuracy']:.4f}  auroc={auroc}\")\n",
    "\n",
    "    if results.get(\"stability_analysis\"):\n",
    "        print(\"\\nDICTIONARY STABILITY:\")\n",
    "        for k_key, stab in results[\"stability_analysis\"].items():\n",
    "            print(\n",
    "                f\"  {k_key}: cosine={stab['mean_cosine_similarity']:.4f}  \"\n",
    "                f\"cv_acc={stab['mean_accuracy']:.4f}\\u00b1{stab['std_accuracy']:.4f}\"\n",
    "            )\n",
    "\n",
    "    print(\"\\nK-SWEEP PARETO:\")\n",
    "    for K in [2, 3, 4, 5, 6, 8, 10]:\n",
    "        key = f\"dots_K{K}\"\n",
    "        if key in results:\n",
    "            r = results[key]\n",
    "            print(f\"  K={K:2d}  acc={r['test_accuracy']:.4f}  \"\n",
    "                  f\"auroc={r.get('test_auroc', 0):.4f}  \"\n",
    "                  f\"splits={r.get('n_splits', '?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "Execute the full DOTS benchmark on the demo dataset (15 examples, reduced K-sweep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full experiment\n",
    "\n",
    "results, examples, train_indices, test_indices, feature_names = (\n",
    "    run_full_experiment(raw_examples)  # DEMO: 15 examples. Original: 200 examples\n",
    ")\n",
    "\n",
    "output_examples = generate_output(\n",
    "    results=results,\n",
    "    raw_examples=examples,\n",
    "    train_indices=train_indices,\n",
    "    test_indices=test_indices,\n",
    ")\n",
    "\n",
    "print_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize the key experimental results: method comparison bar chart, K-sweep analysis, and dictionary stability (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# --- Panel 1: Method Comparison (Test Accuracy) ---\n",
    "ax = axes[0]\n",
    "method_names = []\n",
    "method_accs = []\n",
    "for name, res in results.items():\n",
    "    if name == \"stability_analysis\":\n",
    "        continue\n",
    "    if isinstance(res, dict) and \"test_accuracy\" in res:\n",
    "        label = name.replace(\"_\", \" \").replace(\"figs \", \"FIGS \").replace(\"dots \", \"DOTS \")\n",
    "        method_names.append(label)\n",
    "        method_accs.append(res[\"test_accuracy\"])\n",
    "\n",
    "colors = [\"#2196F3\" if \"DOTS\" in n else \"#FF9800\" if \"FIGS\" in n else \"#4CAF50\" for n in method_names]\n",
    "bars = ax.barh(range(len(method_names)), method_accs, color=colors, edgecolor=\"white\")\n",
    "ax.set_yticks(range(len(method_names)))\n",
    "ax.set_yticklabels(method_names, fontsize=8)\n",
    "ax.set_xlabel(\"Test Accuracy\")\n",
    "ax.set_title(\"Method Comparison\")\n",
    "ax.set_xlim(0, 1.05)\n",
    "for i, v in enumerate(method_accs):\n",
    "    ax.text(v + 0.01, i, f\"{v:.3f}\", va=\"center\", fontsize=7)\n",
    "\n",
    "# --- Panel 2: K-Sweep (Test Accuracy & AUROC vs K) ---\n",
    "ax = axes[1]\n",
    "K_vals = []\n",
    "k_accs = []\n",
    "k_aurocs = []\n",
    "for K in [2, 3, 4, 5, 6, 8, 10]:\n",
    "    key = f\"dots_K{K}\"\n",
    "    if key in results:\n",
    "        K_vals.append(K)\n",
    "        k_accs.append(results[key][\"test_accuracy\"])\n",
    "        k_aurocs.append(results[key].get(\"test_auroc\", 0.5))\n",
    "\n",
    "ax.plot(K_vals, k_accs, \"o-\", color=\"#2196F3\", label=\"Accuracy\", linewidth=2)\n",
    "ax.plot(K_vals, k_aurocs, \"s--\", color=\"#E91E63\", label=\"AUROC\", linewidth=2)\n",
    "ax.set_xlabel(\"Dictionary Size K\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"DOTS K-Sweep\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xticks(K_vals)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 3: Dictionary Stability ---\n",
    "ax = axes[2]\n",
    "stab = results.get(\"stability_analysis\", {})\n",
    "if stab:\n",
    "    k_labels = list(stab.keys())\n",
    "    cosine_means = [stab[k][\"mean_cosine_similarity\"] for k in k_labels]\n",
    "    cv_means = [stab[k][\"mean_accuracy\"] for k in k_labels]\n",
    "    cv_stds = [stab[k][\"std_accuracy\"] for k in k_labels]\n",
    "\n",
    "    x_pos = range(len(k_labels))\n",
    "    ax.bar([p - 0.15 for p in x_pos], cosine_means, 0.3, label=\"Cosine Stability\", color=\"#9C27B0\", alpha=0.8)\n",
    "    ax.bar([p + 0.15 for p in x_pos], cv_means, 0.3, label=\"CV Accuracy\", color=\"#009688\", alpha=0.8,\n",
    "           yerr=cv_stds, capsize=5)\n",
    "    ax.set_xticks(list(x_pos))\n",
    "    ax.set_xticklabels(k_labels)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Dictionary Stability (5-Fold CV)\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"Stability analysis\\nnot available\\n(too few samples)\",\n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12)\n",
    "    ax.set_title(\"Dictionary Stability\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dots_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Results saved to dots_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Part 2 — Full Run (Original Parameters)\n\nThe demo above uses a small subset (15 examples) and reduced parameters so it finishes in seconds. To reproduce the **original results** with the full dataset and all parameter sweeps, uncomment the lines below and re-run the experiment.\n\n> **Note:** The full run may take significantly longer (minutes to hours) depending on data size and hardware.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Uncomment to run with original parameters:\n\n# --- Data: use full dataset (200 examples) instead of demo subset ---\n# # Replace demo_data.json with full_method_out.json\n# FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-54ecf4-dictionary-constrained-oblique-tree-sums/main/experiment_iter2_dots_benchmark/full_method_out.json\"\n# import urllib.request\n# with urllib.request.urlopen(FULL_DATA_URL, timeout=30) as response:\n#     full_data = json.loads(response.read().decode())\n# raw_examples_full = full_data[\"examples\"]  # all 200 examples\n\n# --- Original parameters (edit Section 7 before re-running) ---\n# exp_max_trees = 5                             # Original: 5 (demo used 3)\n# K_values = [2, 3, 4, 5, 6, 8, 10]            # Original: full sweep (demo used [2, 3, 5])\n# n_alternation_rounds = 3                      # Original: 3 (demo used 2)\n\n# --- Re-run with full data and original parameters ---\n# results, examples, train_indices, test_indices, feature_names = (\n#     run_full_experiment(raw_examples_full)     # Full 200 examples\n# )\n# print_summary(results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}