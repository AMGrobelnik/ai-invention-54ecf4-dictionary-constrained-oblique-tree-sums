{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Benchmark Dataset Standardizer for DOTS\n",
    "\n",
    "This notebook demonstrates `data.py`, which loads the **imodels/tabular-benchmark-797-classification**\n",
    "dataset (from the OpenML-797 benchmark suite used in RO-FIGS) and outputs standardized examples\n",
    "for Dictionary-Constrained Oblique Tree Sums (DOTS) evaluation.\n",
    "\n",
    "Each example represents a tabular data row as a binary classification task:\n",
    "- **input**: Feature vector as structured text (44 numeric features: F1R–F22R, F1S–F22S)\n",
    "- **context**: Full feature dictionary + metadata\n",
    "- **output**: Binary classification label (`\"0\"` or `\"1\"`)\n",
    "\n",
    "---\n",
    "\n",
    "**Part 1 — Quick Demo:** Runs on a small 15-example subset loaded from `demo_data.json`.  \n",
    "**Part 2 — Full Version:** Shows the original `main()` pipeline that processes all 200 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Quick Demo\n",
    "\n",
    "### 1.1 Load Demo Data\n",
    "\n",
    "Load a 15-sample demo subset. Tries the GitHub raw URL first (for Colab), then falls back to the local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-54ecf4-dictionary-constrained-oblique-tree-sums/main/dataset_iter1_tabular_bench/demo/demo_data.json\"\n",
    "LOCAL_FILE = \"demo_data.json\"\n",
    "import json, os\n",
    "def load_data():\n",
    "    try:\n",
    "        import urllib.request\n",
    "        with urllib.request.urlopen(GITHUB_RAW_URL, timeout=5) as response:\n",
    "            return json.loads(response.read().decode())\n",
    "    except Exception: pass\n",
    "    if os.path.exists(LOCAL_FILE):\n",
    "        with open(LOCAL_FILE) as f: return json.load(f)\n",
    "    raise FileNotFoundError(\"Could not load data from GitHub or local file\")\n",
    "data = load_data()\n",
    "print(f\"Loaded {len(data['examples'])} examples\")\n",
    "print(f\"Metadata: {data.get('metadata', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Imports\n",
    "\n",
    "Original imports from `data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Constants\n",
    "\n",
    "Original constants from `data.py`. File paths are not used in the demo (data is already loaded above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIDEV-NOTE: Working directory is the script's parent directory\n",
    "# WD = Path(__file__).parent  # Not applicable in notebook\n",
    "# DATASETS_DIR = WD / \"temp\" / \"datasets\"  # Not applicable in notebook\n",
    "# OUTPUT_FILE = WD / \"data_out.json\"  # Not applicable in notebook\n",
    "EXAMPLES_PER_DATASET = 200\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Helper Functions\n",
    "\n",
    "`load_json` loads a JSON file. `clean_feature_name` sanitizes feature names to valid Python identifiers.\n",
    "`format_features_as_text` converts a feature dict into a readable text prompt for the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath: Path) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load a JSON file and return list of records.\"\"\"\n",
    "    with filepath.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def clean_feature_name(name: str) -> str:\n",
    "    \"\"\"Clean feature name to valid Python identifier.\"\"\"\n",
    "    cleaned = name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "    cleaned = \"\".join(c if c.isalnum() or c == \"_\" else \"_\" for c in cleaned)\n",
    "    if cleaned and cleaned[0].isdigit():\n",
    "        cleaned = \"f_\" + cleaned\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def format_features_as_text(features: dict[str, Any]) -> str:\n",
    "    \"\"\"Format feature dict as a readable text description for the input field.\"\"\"\n",
    "    lines = []\n",
    "    for k, v in features.items():\n",
    "        if isinstance(v, float):\n",
    "            lines.append(f\"  {k}: {v:.4f}\")\n",
    "        else:\n",
    "            lines.append(f\"  {k}: {v}\")\n",
    "    return \"Classify the following tabular data sample:\\n\" + \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Core Processor: `process_tabular_benchmark`\n",
    "\n",
    "Processes the imodels/tabular-benchmark-797-classification dataset.\n",
    "This dataset has 44 numeric features (F1R–F22R, F1S–F22S) + a binary target.\n",
    "Already fully numeric — no preprocessing needed beyond field standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tabular_benchmark(\n",
    "    records: list[dict[str, Any]],\n",
    "    n_examples: int,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Process imodels/tabular-benchmark-797-classification dataset.\n",
    "\n",
    "    This dataset has 44 numeric features (F1R..F22R, F1S..F22S) + binary target.\n",
    "    Already fully numeric \\u2014 no preprocessing needed beyond field standardization.\n",
    "    \"\"\"\n",
    "    # AIDEV-NOTE: Remove index columns that are not features\n",
    "    drop_cols = {\"Unnamed: 0.1\", \"Unnamed: 0\", \"target\"}\n",
    "    feature_cols = [c for c in records[0].keys() if c not in drop_cols]\n",
    "    feature_cols_clean = {c: clean_feature_name(c) for c in feature_cols}\n",
    "\n",
    "    # Sample n_examples deterministically\n",
    "    rng = random.Random(RANDOM_SEED)\n",
    "    if len(records) > n_examples:\n",
    "        sampled = rng.sample(records, n_examples)\n",
    "    else:\n",
    "        sampled = records\n",
    "\n",
    "    examples = []\n",
    "    for row in sampled:\n",
    "        target = int(row[\"target\"])\n",
    "        features = {\n",
    "            feature_cols_clean[c]: row[c] for c in feature_cols\n",
    "        }\n",
    "\n",
    "        example = {\n",
    "            \"input\": format_features_as_text(features),\n",
    "            \"context\": {\n",
    "                \"features\": features,\n",
    "                \"n_features\": len(features),\n",
    "                \"task_type\": \"binary_classification\",\n",
    "                \"dataset_source\": \"OpenML-797 benchmark suite\",\n",
    "                \"feature_type\": \"numeric\",\n",
    "                \"preprocessing\": \"none_needed_already_numeric\",\n",
    "            },\n",
    "            \"output\": str(target),\n",
    "            \"dataset\": \"imodels/tabular-benchmark-797-classification\",\n",
    "            \"split\": \"test\",\n",
    "        }\n",
    "        examples.append(example)\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Secondary Processor: `process_churn_prediction`\n",
    "\n",
    "Alternative processor for the scikit-learn/churn-prediction dataset (not used in the final output,\n",
    "but included for completeness). Handles mixed categorical + numeric features with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_churn_prediction(\n",
    "    records: list[dict[str, Any]],\n",
    "    n_examples: int,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Process scikit-learn/churn-prediction dataset.\n",
    "\n",
    "    This dataset has mixed features (categorical + numeric) + binary Churn target.\n",
    "    Preprocessing: one-hot encode categoricals, convert target to 0/1.\n",
    "    \"\"\"\n",
    "    # AIDEV-NOTE: Drop customerID (identifier, not a feature)\n",
    "    id_cols = {\"customerID\"}\n",
    "    target_col = \"Churn\"\n",
    "\n",
    "    # Identify categorical vs numeric features\n",
    "    categorical_cols = [\n",
    "        \"gender\", \"Partner\", \"Dependents\", \"PhoneService\",\n",
    "        \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n",
    "        \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\",\n",
    "        \"StreamingTV\", \"StreamingMovies\", \"Contract\",\n",
    "        \"PaperlessBilling\", \"PaymentMethod\",\n",
    "    ]\n",
    "    numeric_cols = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "    # Encode target: Yes=1, No=0\n",
    "    target_map = {\"Yes\": 1, \"No\": 0}\n",
    "\n",
    "    # Sample n_examples deterministically\n",
    "    rng = random.Random(RANDOM_SEED)\n",
    "    if len(records) > n_examples:\n",
    "        sampled = rng.sample(records, n_examples)\n",
    "    else:\n",
    "        sampled = records\n",
    "\n",
    "    examples = []\n",
    "    for row in sampled:\n",
    "        target_val = row.get(target_col)\n",
    "        if target_val is None:\n",
    "            continue\n",
    "        target = target_map.get(str(target_val), int(target_val) if str(target_val).isdigit() else 0)\n",
    "\n",
    "        # Build feature dict with one-hot encoding for categoricals\n",
    "        features: dict[str, Any] = {}\n",
    "\n",
    "        # Numeric features\n",
    "        for col in numeric_cols:\n",
    "            val = row.get(col)\n",
    "            if val is None or val == \"\" or val == \" \":\n",
    "                # AIDEV-NOTE: Impute missing with 0.0 (median would require full dataset scan)\n",
    "                features[clean_feature_name(col)] = 0.0\n",
    "            else:\n",
    "                try:\n",
    "                    features[clean_feature_name(col)] = float(val)\n",
    "                except (ValueError, TypeError):\n",
    "                    features[clean_feature_name(col)] = 0.0\n",
    "\n",
    "        # One-hot encode categorical features\n",
    "        for col in categorical_cols:\n",
    "            val = str(row.get(col, \"Unknown\"))\n",
    "            ohe_name = clean_feature_name(f\"{col}_{val}\")\n",
    "            # AIDEV-NOTE: Set current category to 1, others implicitly 0 in context\n",
    "            features[ohe_name] = 1\n",
    "\n",
    "        # Build original features dict (before encoding) for context\n",
    "        original_features = {}\n",
    "        for col in numeric_cols + categorical_cols:\n",
    "            if col not in id_cols:\n",
    "                original_features[col] = row.get(col)\n",
    "\n",
    "        example = {\n",
    "            \"input\": format_features_as_text(features),\n",
    "            \"context\": {\n",
    "                \"features\": features,\n",
    "                \"original_features\": original_features,\n",
    "                \"n_features_original\": len(numeric_cols) + len(categorical_cols),\n",
    "                \"n_features_encoded\": len(features),\n",
    "                \"task_type\": \"binary_classification\",\n",
    "                \"dataset_source\": \"scikit-learn/churn-prediction (IBM Telco)\",\n",
    "                \"feature_type\": \"mixed_categorical_numeric\",\n",
    "                \"preprocessing\": \"one_hot_encoded_categoricals\",\n",
    "                \"target_encoding\": {\"Yes\": 1, \"No\": 0},\n",
    "            },\n",
    "            \"output\": str(target),\n",
    "            \"dataset\": \"scikit-learn/churn-prediction\",\n",
    "            \"split\": \"train\",\n",
    "        }\n",
    "        examples.append(example)\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Run on Demo Data\n",
    "\n",
    "Use the pre-processed examples from `demo_data.json` and run the summary statistics\n",
    "from the original `main()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded demo examples directly (already processed by data.py)\n",
    "tabular_examples = data[\"examples\"][:15]  # DEMO: use 15 examples. Original: all 200 examples\n",
    "\n",
    "print(f\"Total examples: {len(tabular_examples)}\")\n",
    "\n",
    "# Summary statistics (from original main())\n",
    "targets = [int(e[\"output\"]) for e in tabular_examples]\n",
    "class_1_frac = sum(targets) / len(targets) if targets else 0\n",
    "print(f\"\\n  tabular-benchmark-797-classification (SELECTED):\")\n",
    "print(f\"    Source: imodels/tabular-benchmark-797-classification (OpenML-797 suite)\")\n",
    "print(f\"    Task: binary classification\")\n",
    "print(f\"    Features: {tabular_examples[0]['context']['n_features']} (all numeric)\")\n",
    "print(f\"    Class balance: {class_1_frac:.3f} (class=1)\")\n",
    "print(f\"    Examples: {len(tabular_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Inspect an Example\n",
    "\n",
    "Show the structure of a single processed example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = tabular_examples[0]\n",
    "print(\"=== Example 0 ===\")\n",
    "print(ex[\"input\"][:300], \"...\")\n",
    "print(f\"\\nOutput (target): {ex['output']}\")\n",
    "print(f\"Dataset: {ex['dataset']}\")\n",
    "print(f\"Split: {ex['split']}\")\n",
    "print(f\"Context keys: {list(ex['context'].keys())}\")\n",
    "print(f\"Number of features: {ex['context']['n_features']}\")\n",
    "print(f\"Feature names (first 6): {list(ex['context']['features'].keys())[:6]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2 — Full Run — Original Parameters\n\nThe cell below shows how to restore all original parameters to reproduce the full pipeline results.\nUncommenting those lines will use the complete dataset (200 examples) instead of the 15-example demo subset.\n\nThe subsequent cell contains the original `main()` function that loads raw records from disk,\nprocesses all 200 examples via `process_tabular_benchmark`, and writes `data_out.json`.\nThis requires the raw dataset file at `temp/datasets/full_imodels_tabular-benchmark-797-classification_test.json`.\n\n> **Note:** The full run may take significantly longer depending on data size and available resources."
  },
  {
   "cell_type": "code",
   "source": "# Uncomment to run with original parameters:\n# tabular_examples = data[\"examples\"]  # all 200 examples (DEMO used: 15)\n# EXAMPLES_PER_DATASET = 200  # Original: 200 (already set above)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main processing pipeline.\n",
    "\n",
    "    AIDEV-NOTE: Only outputs the BEST dataset (tabular-benchmark-797-classification).\n",
    "    Selected over churn-prediction because it's directly from the OpenML-797 benchmark\n",
    "    suite used in RO-FIGS, has pure numeric features, and needs no preprocessing.\n",
    "    \"\"\"\n",
    "    # NOTE: This cell is for reference only. It requires raw dataset files not included in the demo.\n",
    "    # To run the full pipeline, use: python data.py\n",
    "    raise RuntimeError(\"Full pipeline requires raw dataset files. Run 'python data.py' instead.\")\n",
    "\n",
    "    DATASETS_DIR = Path(\"temp\") / \"datasets\"\n",
    "    OUTPUT_FILE = Path(\"data_out.json\")\n",
    "\n",
    "    print(f\"Loading datasets from: {DATASETS_DIR}\")\n",
    "\n",
    "    # Load the selected best dataset\n",
    "    tabular_file = DATASETS_DIR / \"full_imodels_tabular-benchmark-797-classification_test.json\"\n",
    "    tabular_records = load_json(tabular_file)\n",
    "    print(f\"  tabular-benchmark-797: {len(tabular_records)} records loaded\")\n",
    "\n",
    "    # Process selected dataset (200 examples)\n",
    "    tabular_examples = process_tabular_benchmark(\n",
    "        records=tabular_records,\n",
    "        n_examples=EXAMPLES_PER_DATASET,\n",
    "    )\n",
    "    print(f\"  tabular-benchmark-797: {len(tabular_examples)} examples extracted\")\n",
    "\n",
    "    # Build output matching exp_sel_data_out.json schema\n",
    "    output = {\"examples\": tabular_examples}\n",
    "\n",
    "    # Save to full_data_out.json\n",
    "    with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nOutput saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Total examples: {len(tabular_examples)}\")\n",
    "\n",
    "    # Summary statistics\n",
    "    targets = [int(e[\"output\"]) for e in tabular_examples]\n",
    "    class_1_frac = sum(targets) / len(targets) if targets else 0\n",
    "    print(f\"\\n  tabular-benchmark-797-classification (SELECTED):\")\n",
    "    print(f\"    Source: imodels/tabular-benchmark-797-classification (OpenML-797 suite)\")\n",
    "    print(f\"    Task: binary classification\")\n",
    "    print(f\"    Features: {tabular_examples[0]['context']['n_features']} (all numeric)\")\n",
    "    print(f\"    Class balance: {class_1_frac:.3f} (class=1)\")\n",
    "    print(f\"    Examples: {len(tabular_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Feature distributions and class balance for the demo subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract feature matrix and labels\n",
    "feature_names = list(tabular_examples[0][\"context\"][\"features\"].keys())\n",
    "X = np.array([[ex[\"context\"][\"features\"][f] for f in feature_names] for ex in tabular_examples])\n",
    "y = np.array([int(ex[\"output\"]) for ex in tabular_examples])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# --- Plot 1: Class distribution ---\n",
    "counts = [int(np.sum(y == 0)), int(np.sum(y == 1))]\n",
    "bars = axes[0, 0].bar([\"Class 0\", \"Class 1\"], counts, color=[\"#4C72B0\", \"#DD8452\"])\n",
    "axes[0, 0].set_title(\"Class Distribution\")\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "for bar, c in zip(bars, counts):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, c + 0.15, str(c),\n",
    "                    ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "# --- Plot 2: Mean feature values by class ---\n",
    "mean_0 = X[y == 0].mean(axis=0) if np.any(y == 0) else np.zeros(X.shape[1])\n",
    "mean_1 = X[y == 1].mean(axis=0) if np.any(y == 1) else np.zeros(X.shape[1])\n",
    "x_pos = np.arange(len(feature_names))\n",
    "axes[0, 1].bar(x_pos - 0.2, mean_0, 0.4, label=\"Class 0\", color=\"#4C72B0\", alpha=0.8)\n",
    "axes[0, 1].bar(x_pos + 0.2, mean_1, 0.4, label=\"Class 1\", color=\"#DD8452\", alpha=0.8)\n",
    "axes[0, 1].set_title(\"Mean Feature Values by Class\")\n",
    "axes[0, 1].set_xlabel(\"Feature Index\")\n",
    "axes[0, 1].set_ylabel(\"Mean Value\")\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "axes[0, 1].set_xticks(x_pos[::4])\n",
    "axes[0, 1].set_xticklabels([feature_names[i] for i in range(0, len(feature_names), 4)],\n",
    "                           rotation=45, ha=\"right\", fontsize=7)\n",
    "\n",
    "# --- Plot 3: Feature value spread (box plot, first 10 features) ---\n",
    "bp = axes[1, 0].boxplot(X[:, :10], labels=feature_names[:10], patch_artist=True)\n",
    "for patch in bp[\"boxes\"]:\n",
    "    patch.set_facecolor(\"#A1C9F4\")\n",
    "axes[1, 0].set_title(\"Feature Value Spread (First 10 Features)\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# --- Plot 4: Per-feature std deviation ---\n",
    "stds = X.std(axis=0)\n",
    "axes[1, 1].barh(feature_names, stds, color=\"#FFBE7A\", edgecolor=\"#DD8452\")\n",
    "axes[1, 1].set_title(\"Feature Std. Deviation\")\n",
    "axes[1, 1].set_xlabel(\"Std Dev\")\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].tick_params(axis=\"y\", labelsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"demo_visualization.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Value range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"  Class counts: 0={counts[0]}, 1={counts[1]}\")\n",
    "print(f\"  Class 1 fraction: {counts[1]/sum(counts):.3f}\")\n",
    "print(f\"  Feature names: {feature_names[:6]} ... ({len(feature_names)} total)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}